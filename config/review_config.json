{
    "Customer": "",
    "Endpoint": "",
    "Sections": {
        "NodeDetails": {
            "SQL": "WITH node_slice AS\n(\n  SELECT node,\n         COUNT(1) AS slice_count\n  FROM stv_slices\n  GROUP BY node\n),\nnode_storage_utilization AS\n(\n  SELECT node::text AS node,\n         (1.0 * used / capacity)::NUMERIC(8,4) * 100 AS storage_utilization_pct,\n         1.0 * capacity/1000 AS storage_capacity_gb,\n         1.0 * used/1000 AS storage_used_gb\n  FROM stv_node_storage_capacity\n)\nSELECT CASE\n         WHEN capacity = 190633 AND NOT is_nvme THEN 'dc1.large'\n         WHEN capacity = 380319 THEN 'dc1.8xlarge'\n         WHEN capacity = 190633 AND is_nvme THEN 'dc2.large'\n         WHEN capacity = 760956 THEN 'dc2.8xlarge'\n         WHEN capacity = 726296 THEN 'dc2.8xlarge'\n         WHEN capacity = 952455 THEN 'ds2.xlarge'\n         WHEN capacity = 945026 THEN 'ds2.8xlarge'\n         WHEN capacity = 954367 AND part_count = 1 THEN 'ra3.xlplus'\n         WHEN capacity = 3339176 AND part_count = 1 THEN 'ra3.4xlarge'\n         WHEN capacity = 3339176 AND part_count = 4 THEN 'ra3.16xlarge'\n         ELSE 'unknown'\n       END AS node_type,\n       s.node,\n       slice_count,\n       storage_utilization_pct,\n       storage_capacity_gb,\n       storage_used_gb\nFROM (SELECT p.host AS node,\n             p.capacity,\n             p.mount LIKE '/dev/nvme%' AS is_nvme,\n             COUNT(1) AS part_count\n      FROM stv_partitions p\n      WHERE p.host = p.owner\n      GROUP BY 1,\n               2,\n               3) AS s\n  INNER JOIN node_slice n ON (s.node = n.node)\n  INNER JOIN node_storage_utilization ns ON (s.node = ns.node)\nORDER by 2;",
            "Signals": [
                {
                    "Signal": "exceeds the recommended storage threshold of 70%",
                    "Criteria": "storage_utilization_pct > 70 and (node_type.str.startswith('dc') or node_type.str.startswith('ds'))",
                    "PopulationCriteria": "",
                    "PopulationName": "nodes",
                    "Recommendation": [
                        "5",
                        "1",
                        "6",
                        "4",
                        "2"
                    ]
                },
                {
                    "Signal": "has under-utilized storage",
                    "Criteria": "storage_utilization_pct < 40 and not node_type.str.startswith('ra3') ",
                    "PopulationCriteria": "",
                    "PopulationName": "nodes",
                    "Recommendation": [
                        "12"
                    ]
                },
                {
                    "Signal": "has 10% data skew",
                    "Criteria": "select * from df where (storage_utilization_pct - (select  min(storage_utilization_pct) from df)) *100 > 10",
                    "PopulationCriteria": "",
                    "PopulationName": "nodes",
                    "Recommendation": [
                        "8"
                    ]
                },
                {
                    "Signal": "are using a legacy node type",
                    "Criteria": "node_type.str.startswith('dc1') or node_type.str.startswith('ds2')",
                    "PopulationCriteria": "",
                    "PopulationName": "nodes",
                    "Recommendation": [
                        "1"
                    ]
                }
            ],
            "Observations": [
                {
                    "Observation": "[SA to Validate] CloudWatch metric showing CPU usage is consistently above 80%, ATO and other automated tasks such as Auto Vacuum or Auto Analyze may not be triggered",
                    "Recommendation": [
                        "5",
                        "26"
                    ]
                },
                {
                    "Observation": "[SA to Validate] Internal metric showing SSD Cache hit rate shows frequent misses indicating querying of non-cached data",
                    "Recommendation": [
                        "5"
                    ]
                },
                {
                    "Observation": "[SA to Validate] CloudWatch metric showing Disk Usage has sudden peaks and drops indicating possible disk spill",
                    "Recommendation": [
                        "5"
                    ]
                },
                {
                    "Observation": "[SA to Validate] CloudWatch metric showing number of connections is consistently high",
                    "Recommendation": [
                        "36"
                    ]
                },
                {
                    "Observation": "[SA to Validate] Production cluster is on 'Current' track.  ",
                    "Recommendation": [
                        "37"
                    ]
                },
                {
                    "Observation": "[SA to Validate] On-demand pricing model",
                    "Recommendation": [
                        "35"
                    ]
                }
            ]
        },
        "WLMConfig": {
            "SQL": "SELECT c.wlm_mode,\n       scc.service_class::text AS service_class_id,\n       CASE\n         WHEN scc.service_class BETWEEN 1 AND 4 THEN 'System'\n         WHEN scc.service_class = 5 THEN 'Superuser'\n         WHEN scc.service_class BETWEEN 6 AND 13 THEN 'Manual WLM'\n         WHEN scc.service_class = 14 THEN 'SQA'\n         WHEN scc.service_class = 15 THEN 'Redshift Maintenance'\n         WHEN scc.service_class BETWEEN 100 AND 107 THEN 'Auto WLM'\n       END AS service_class_category,\n       trim(scc.name) AS queue_name,\n       CASE\n         WHEN scc.num_query_tasks = -1 THEN 'auto'\n         ELSE scc.num_query_tasks::text\n       END AS slots,\n       CASE\n         WHEN scc.query_working_mem = -1 THEN 'auto'\n         ELSE scc.query_working_mem::text\n       END AS query_working_memory_mb_per_slot,\n       nvl(cast(ROUND(((scc.num_query_tasks*scc.query_working_mem)::NUMERIC/ mem.total_memory_mb::NUMERIC)*100,0)::NUMERIC(38,4) as varchar(12)),'auto') cluster_memory_pct,\n       scc.max_execution_time AS query_timeout,\n       trim(scc.concurrency_scaling) AS concurrency_scaling,\n       trim(scc.query_priority) AS queue_priority,\n       nvl(qc.qmr_rule_count,0) AS qmr_rule_count,\n       CASE\n         WHEN qmr.qmr_rule IS NOT NULL THEN 'Y'\n         ELSE 'N'\n       END AS is_queue_evictable,\n       LISTAGG(DISTINCT TRIM(qmr.qmr_rule),',') within group(ORDER BY rule_name) qmr_rule,\n       LISTAGG(TRIM(cnd.condition),', ') condition\nFROM stv_wlm_service_class_config scc\n  INNER JOIN stv_wlm_classification_config cnd ON scc.service_class = cnd.action_service_class\n  CROSS JOIN (SELECT CASE\n                       WHEN COUNT(1) > 0 THEN 'auto'\n                       ELSE 'manual'\n                     END AS wlm_mode\n              FROM stv_wlm_service_class_config\n              WHERE service_class >= 100) c\n  CROSS JOIN (SELECT SUM(num_query_tasks*query_working_mem) AS total_memory_mb\n              FROM stv_wlm_service_class_config\n              WHERE service_class BETWEEN 6 AND 13) mem\n  LEFT OUTER JOIN (SELECT service_class,\n                          COUNT(DISTINCT rule_name) AS qmr_rule_count\n                   FROM stv_wlm_qmr_config\n                   GROUP BY service_class) qc ON (scc.service_class = qc.service_class)\n  LEFT OUTER JOIN (SELECT service_class,\n                          rule_name,\n                          rule_name || ':' || '[' || action || '] ' || metric_name || metric_operator || CAST(metric_value AS VARCHAR(256)) qmr_rule\n                   FROM stv_wlm_qmr_config) qmr ON scc.service_class = qmr.service_class\nWHERE scc.service_class > 4\nGROUP BY 1,\n         2,\n         3,\n         4,\n         5,\n         6,\n         7,\n         8,\n         9,\n         10,\n         11,\n         12\nORDER BY 2 ASC;",
            "Signals": [
                {
                    "Signal": "queues do not have total memory = 100",
                    "Criteria": "select sum(cluster_memory_pct) from df || < 100",
                    "PopulationCriteria": "wlm_mode='manual' and service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "16"
                    ]
                },
                {
                    "Signal": "queues have total concurrency > 20",
                    "Criteria": "select sum(slots) from df where service_class_id not in (5,14,15) || > 20",
                    "PopulationCriteria": "wlm_mode='manual' and service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "17"
                    ]
                },
                {
                    "Signal": "uses single WLM queue",
                    "Criteria": "select count(1) from df where service_class_category = 'Manual WLM' OR  service_class_category = 'Auto WLM' || == 1",
                    "PopulationCriteria": "service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "18"
                    ]
                },
                {
                    "Signal": "uses manual WLM",
                    "Criteria": "select count(1) from df where wlm_mode='manual' || > 0",
                    "PopulationCriteria": "wlm_mode='manual' and service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "15"
                    ]
                },
                {
                    "Signal": "Concurrency scaling is not enabled",
                    "Criteria": "select count(1) from df where concurrency_scaling = 'auto' || == 0",
                    "PopulationCriteria": "service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "19"
                    ]
                },
                {
                    "Signal": "short query acceleration (SQA) is not enabled",
                    "Criteria": "select count(1) from df where service_class_category != 'SQA' || ==1",
                    "PopulationCriteria": "service_class_id <> 5 and service_class_id <> 15",
                    "Recommendation": [
                        "20"
                    ]
                },
                {
                    "Signal": "all query queues having the same query priority",
                    "Criteria": "select count(distinct queue_priority) from df || == 1",
                    "PopulationCriteria": "service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "21"
                    ]
                },
                {
                    "Signal": "no query monitoring rules (QMR) defined",
                    "Criteria": "select sum(qmr_rule_count) from df || == 0",
                    "PopulationCriteria": "service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "22"
                    ]
                },
                {
                    "Signal": "no QMR defined for query_execution_time metric",
                    "Criteria": "select count(1) from df where qmr_rule not like '%query_execution_time%' || ==1 ",
                    "PopulationCriteria": "service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "22"
                    ]
                },
                {
                    "Signal": "no QMR defined for query_temp_blocks_to_disk metric",
                    "Criteria": "select count(1) from df where qmr_rule not like '%query_temp_blocks_to_disk%' || ==1",
                    "PopulationCriteria": "service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "22"
                    ]
                },
                {
                    "Signal": "no QMR defined for spectrum_scan_size_mb or spectrum_scan_row_count metric",
                    "Criteria": "select count(1) from df where qmr_rule not like '%spectrum_scan%' || ==1",
                    "PopulationCriteria": "service_class_id <> 5 and service_class_id <> 14 and service_class_id <> 15",
                    "Recommendation": [
                        "22"
                    ]
                }
            ],
            "Observations": []
        },
        "WLMandCommit": {
            "SQL": "SELECT IQ.*,\n       (IQ.wlm_queue_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) AS pct_wlm_queue_time,\n       (IQ.exec_only_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) AS pct_exec_only_time,\n       (IQ.commit_queue_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) pct_commit_queue_time,\n       (IQ.commit_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) pct_commit_time\nFROM (SELECT TRUNC(b.starttime) AS DAY,\n             d.service_class,\n             rtrim(s.name) as queue_name,\n             c.node,\n             COUNT(DISTINCT c.xid) AS count_commit_xid,\n             SUM(datediff ('microsec',d.service_class_start_time,c.endtime)*0.001)::numeric(38,4) AS wlm_start_commit_time_ms,\n             SUM(datediff ('microsec',d.queue_start_time,d.queue_end_time)*0.001)::numeric(38,4) AS wlm_queue_time_ms,\n             SUM(datediff ('microsec',b.starttime,b.endtime)*0.001)::numeric(38,4) AS exec_only_time_ms,\n             SUM(datediff ('microsec',c.startwork,c.endtime)*0.001)::numeric(38,4) commit_time_ms,\n             SUM(datediff ('microsec',DECODE(c.startqueue,'2000-01-01 00:00:00',c.startwork,c.startqueue),c.startwork)*0.001)::numeric(38,4) commit_queue_time_ms\n      FROM stl_query b,\n           stl_commit_stats c,\n           stl_wlm_query d,\n           stv_wlm_service_class_config s\n      WHERE b.xid = c.xid\n      AND   b.query = d.query\n      AND   c.xid > 0\n      AND d.service_class = s.service_class\n      GROUP BY 1,2,3,4\n      ORDER BY 1,2,3,4) IQ;",
            "Signals": [
                {
                    "Signal": "queue/nodes have heavy WLM queuing",
                    "Criteria": "pct_wlm_queue_time > 5",
                    "PopulationCriteria": "node != -1 ",
                    "Recommendation": [
                        "19",
                        "26"
                    ]
                },
                {
                    "Signal": "queue/nodes heavy commit queuing",
                    "Criteria": "pct_commit_queue_time > 5",
                    "PopulationCriteria": "node != -1",
                    "Recommendation": [
                        "23",
                        "26"
                    ]
                }
            ],
            "Observations": []
        },
        "QueueHealth": {
            "SQL": "with workload as\n(\nselect trim(sq.\"database\") as dbname\n      ,case \n\t     when sq.concurrency_scaling_status = 1 then 'burst'\t        \n\t\t\t else 'main' end as concurrency_scaling_status \n      ,case \n       when sl.source_query is not null then 'result_cache'       \n\t\t\t else rtrim(swsc.name) end as queue_name \n\t\t\t,swq.service_class \n      ,case\n       when swq.service_class between 1 and 4 then 'System'\n       when swq.service_class = 5 then 'Superuser'\n       when swq.service_class between 6 and 13 then'Manual WLM queues'\n       when swq.service_class = 14 then 'SQA'\n       when swq.service_class = 15 then 'Redshift Maintenance'\n       when swq.service_class between 100 and 107 then 'Auto WLM'\n       end as service_class_category \t  \n      ,sq.query as query_id\n      ,case \n         when regexp_instr(sq.querytxt, '(padb_|pg_internal)'             ) then 'OTHER'\n         when regexp_instr(sq.querytxt, '([uU][nN][dD][oO][iI][nN][gG]) ' ) then 'SYSTEM'\n         when regexp_instr (sq.querytxt,'([aA][uU][tT][oO][mM][vV])'      ) then 'AUTOMV'\n         when regexp_instr(sq.querytxt, '[uU][nN][lL][oO][aA][dD]'        ) then 'UNLOAD'\n         when regexp_instr(sq.querytxt, '[cC][uU][rR][sS][oO][rR] '       ) then 'CURSOR'\n         when regexp_instr(sq.querytxt, '[fF][eE][tT][cC][hH] '           ) then 'CURSOR'\n         WHEN regexp_instr (sq.querytxt,'[cC][rR][eE][aA][tT][eE] '       ) then 'CTAS'\n         when regexp_instr(sq.querytxt, '[dD][eE][lL][eE][tT][eE] '       ) then 'DELETE'\n         when regexp_instr(sq.querytxt, '[uU][pP][dD][aA][tT][eE] '       ) then 'UPDATE'\n         when regexp_instr(sq.querytxt, '[iI][nN][sS][eE][rR][tT] '       ) then 'INSERT'\n         when regexp_instr(sq.querytxt, '[vV][aA][cC][uU][uU][mM][ :]'    ) then 'VACUUM'\n         when regexp_instr(sq.querytxt, '[aA][nN][aA][lL][yY][zZ][eE] '   ) then 'ANALYZE'\t\t \n         when regexp_instr(sq.querytxt, '[sS][eE][lL][eE][cC][tT] '       ) then 'SELECT'\n         when regexp_instr(sq.querytxt, '[cC][oO][pP][yY] '               ) then 'COPY'\n         else 'OTHER' \n       end as query_type \n      ,date_trunc('hour',sq.starttime) as workload_exec_hour\n      ,nvl(swq.est_peak_mem/1024.0/1024.0/1024.0,0.0) as est_peak_mem_gb\n      ,decode(swq.final_state, 'Completed',decode(swr.action, 'abort',0,decode(sq.aborted,0,1,0)),'Evicted',0,null,decode(sq.aborted,0,1,0)::int) as is_completed\n      ,decode(swq.final_state, 'Completed',decode(swr.action, 'abort',1,0),'Evicted',1,null,0) as is_evicted_aborted\n      ,decode(swq.final_state, 'Completed',decode(swr.action, 'abort',0,decode(sq.aborted,1,1,0)),'Evicted',0,null,decode(sq.aborted,1,1,0)::int) as is_user_aborted\n\t    ,case when sl.from_sp_call is not null then 1 else 0 end as from_sp_call\n\t    ,case when alrt.num_events is null then 0 else alrt.num_events end as alerts\n\t    ,case when dsk.num_diskbased > 0 then 1 else 0 end as is_query_diskbased\n\t    ,nvl(c.num_compile_segments,0) as num_compile_segments\n      ,cast(case when sqms.query_queue_time is null then 0 else sqms.query_queue_time end as decimal(26,6)) as query_queue_time_secs\n\t    ,nvl(c.max_compile_time_secs,0) as max_compile_time_secs\n\t    ,sl.starttime\n\t    ,sl.endtime\n\t    ,sl.elapsed\n      ,cast(sl.elapsed * 0.000001 as decimal(26,6)) as query_execution_time_secs\t\n      ,sl.elapsed * 0.000001 - nvl(c.max_compile_time_secs,0)  - nvl(sqms.query_queue_time,0) as actual_execution_time_secs\t  \n      ,case when sqms.query_temp_blocks_to_disk is null then 0 else sqms.query_temp_blocks_to_disk end as query_temp_blocks_to_disk_mb\n      ,cast(case when sqms.query_cpu_time is null then 0 else sqms.query_cpu_time end as decimal(26,6)) as query_cpu_time_secs \n\t    ,nvl(sqms.scan_row_count,0) as scan_row_count\n      ,nvl(sqms.return_row_count,0) as return_row_count\n      ,nvl(sqms.nested_loop_join_row_count,0) as nested_loop_join_row_count\n      ,nvl(uc.usage_limit_count,0) as cs_usage_limit_count\n  from stl_query sq\n  inner join svl_qlog sl on (sl.userid = sq.userid and sl.query = sq.query)\n  left outer join svl_query_metrics_summary sqms on (sqms.userid = sq.userid and sqms.query = sq.query)\t\t\t\t\t\n  left outer join stl_wlm_query swq on (sq.userid = swq.userid and sq.query = swq.query)\n  left outer join stl_wlm_rule_action swr on (sq.userid = swr.userid and sq.query = swr.query and swq.service_class = swr.service_class)\n  left outer join stv_wlm_service_class_config swsc on (swsc.service_class = swq.service_class)\n  left outer join (select sae.query\n                         ,cast(1 as integer) as num_events\n                     from svcs_alert_event_log sae\n                   group by sae.query) as alrt on (alrt.query = sq.query)  \n  left outer join (select sqs.userid\n                         ,sqs.query\n                         ,1 as num_diskbased\n                     from svcs_query_summary sqs    \n                    where sqs.is_diskbased = 't'\n                   group by sqs.userid, sqs.query\n                   ) as dsk on (dsk.userid = sq.userid and dsk.query = sq.query)  \n  left outer join (select userid, xid,  pid, query\n                         ,max(datediff(ms, starttime, endtime)*1.0/1000) as max_compile_time_secs\n\t                     ,sum(compile) as num_compile_segments\n                     from svcs_compile\n                   group by userid, xid,  pid, query\n                  ) c on (c.userid = sq.userid and c.xid = sq.xid and c.pid = sq.pid and c.query = sq.query)                 \n  left outer join (select query,xid,pid\n                          ,count(1) as usage_limit_count\n                      from stl_usage_control \n                     where feature_type = 'CONCURRENCY_SCALING'\n                   group by query, xid, pid) uc on (uc.xid = sq.xid and uc.pid = sq.pid and uc.query = sq.query)                  \t   \n  where sq.userid <> 1 \n    and sq.querytxt not like 'padb_fetch_sample%'\n    and sq.starttime >= dateadd(day,-7,current_date)\n)\nselect workload_exec_hour\n      ,service_class_category\n      ,service_class\n      ,queue_name\n      ,concurrency_scaling_status\n      ,dbname\n      ,query_type\n      ,sum(is_completed) + sum(is_user_aborted) + sum(is_evicted_aborted) as total_query_count\n      ,sum(is_completed) as completed_query_count\n      ,sum(is_user_aborted) as user_aborted_count\n      ,sum(is_evicted_aborted) as wlm_evicted_count\n      ,round(sum(est_peak_mem_gb),4) as total_est_peak_mem_gb\n      ,sum(is_query_diskbased) as total_disk_spill_count\n      ,sum(num_compile_segments) as total_compile_count\n      ,round(sum(query_temp_blocks_to_disk_mb/1024.0),4) as total_disk_spill_gb\n      ,sum(alerts) as total_query_alert_count\n      ,sum(from_sp_call) as total_called_proc_count\n\t  ,avg(query_execution_time_secs) as avg_query_execution_time_secs\n\t  ,max(query_execution_time_secs) as max_query_execution_time_secs\n      ,sum(query_execution_time_secs) as total_query_execution_time_secs\n\t  ,avg(max_compile_time_secs) as avg_compile_time_secs\n\t  ,max(max_compile_time_secs) as max_compile_time_secs\n      ,sum(max_compile_time_secs) as total_compile_time_secs\n\t  ,avg(query_queue_time_secs) as avg_query_queue_time_secs\n\t  ,max(query_queue_time_secs) as max_query_queue_time_secs\n      ,sum(query_queue_time_secs) as total_query_queue_time_secs\n\t  ,avg(actual_execution_time_secs) as avg_actual_execution_time_secs\n      ,sum(actual_execution_time_secs) as total_actual_execution_time_secs\n      ,sum(query_cpu_time_secs) as total_query_cpu_time_secs\n      ,sum(cs_usage_limit_count) as total_cs_usage_limit_count\n      ,sum(scan_row_count) as total_scan_row_count\n      ,sum(return_row_count) as total_return_row_count\n      ,sum(nested_loop_join_row_count) as total_nl_join_row_count\n  from workload\n  group by 1,2,3,4,5,6,7\n  order by 1,2,3,4,5,6,7;",
            "Signals": [
                {
                    "Signal": "hours/queues where queries spilled to disk",
                    "Criteria": "total_disk_spill_count > 10 ",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "22",
                        "5",
                        "11",
                        "24"
                    ]
                },
                {
                    "Signal": "hours/queues with a high compile count",
                    "Criteria": "total_compile_count > 10",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "25"
                    ]
                },
                {
                    "Signal": "hours/queues with a high nl join row count",
                    "Criteria": "total_nl_join_row_count > 1000",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "9"
                    ]
                }
            ],
            "Observations": []
        },
        "UsagePattern": {
            "SQL": "WITH workload_profile AS\n(\n  SELECT trim(t.\"database\") AS dbname,\n         s.statement_type,\n         CAST(CASE \n              WHEN regexp_instr (s.statement_text,'(padb_|pg_internal)'              ) THEN 'SYSTEM' \n              WHEN regexp_instr (s.statement_text,'([uU][nN][dD][oO][iI][nN][gG]) '  ) THEN 'SYSTEM' \n              WHEN regexp_instr (s.statement_text,'([aA][uU][tT][oO][mM][vV])'       ) THEN 'AUTOMV' \n              WHEN regexp_instr (s.statement_text,'[uU][nN][lL][oO][aA][dD]'         ) THEN 'UNLOAD' \n              WHEN regexp_instr (s.statement_text,'[cC][uU][rR][sS][oO][rR] '        ) THEN 'CURSOR' \n              WHEN regexp_instr (s.statement_text,'[cC][rR][eE][aA][tT][eE] '        ) THEN 'DDL' \n              WHEN regexp_instr (s.statement_text,'[aA][lL][tT][eE][rR] '            ) THEN 'DDL' \n              WHEN regexp_instr (s.statement_text,'[dD][rR][oO][pP] '                ) THEN 'DDL' \n              WHEN regexp_instr (s.statement_text,'[tT][rR][uU][nN][cC][aA][tT][eE] ') THEN 'DDL' \n              WHEN regexp_instr (s.statement_text,'[cC][aA][lL][lL] '                ) THEN 'CALL_PROCEDURE' \n              WHEN regexp_instr (s.statement_text,'[gG][rR][aA][nN][tT] '            ) THEN 'DCL' \n              WHEN regexp_instr (s.statement_text,'[rR][eE][vV][oO][kK][eE] '        ) THEN 'DCL' \n              WHEN regexp_instr (s.statement_text,'[fF][eE][tT][cC][hH] '            ) THEN 'CURSOR' \n              WHEN regexp_instr (s.statement_text,'[dD][eE][lL][eE][tT][eE] '        ) THEN 'DELETE'\n              WHEN regexp_instr (s.statement_text,'[uU][pP][dD][aA][tT][eE] '        ) THEN 'UPDATE' \n              WHEN regexp_instr (s.statement_text,'[iI][nN][sS][eE][rR][tT] '        ) THEN 'INSERT' \n              WHEN regexp_instr (s.statement_text,'[vV][aA][cC][uU][uU][mM][ :]'     ) THEN 'VACUUM' \n              WHEN regexp_instr (s.statement_text,'[aA][nN][aA][lL][yY][zZ][eE] '    ) THEN 'ANALYZE' \n              WHEN regexp_instr (s.statement_text,'[sS][eE][lL][eE][cC][tT] '        ) THEN 'SELECT' \n              WHEN regexp_instr (s.statement_text,'[cC][oO][pP][yY] '                ) THEN 'COPY' \n              ELSE 'OTHER' \n              END AS VARCHAR(32)) AS query_type,\n         DATEPART(HOUR,t.starttime) query_hour,\n\t\t t.starttime::DATE as query_date,\n         ROUND(SUM(duration_secs),1) query_duration_secs,\n         COUNT(*) query_count\n  FROM (SELECT s.userid,\n               s.type AS statement_type,\n               s.xid,\n               s.pid,\n               DATEDIFF(milliseconds,starttime,endtime)::NUMERIC(38,4) / 1000 AS duration_secs,\n               CAST(REPLACE(rtrim(s.text),'\\n','') AS VARCHAR(800)) AS statement_text\n        FROM svl_statementtext s\n        WHERE s.sequence = 0\n        AND   s.starttime >= DATEADD(DAY,-7,CURRENT_DATE)\n       ) s\n    LEFT OUTER JOIN stl_query t\n                 ON (s.xid = t.xid\n                AND s.pid = t.pid\n                AND s.userid = t.userid)\n  WHERE s.statement_text NOT LIKE '%volt_tt_%'\n  GROUP BY 1,2,3,4,5\n)\nSELECT wp.dbname,\n       wp.query_date,\n       wp.query_hour,\n       MAX(NVL(CASE WHEN wp.query_type = 'SELECT' THEN query_count ELSE 0 END,0))::bigint AS select_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'SELECT' THEN query_duration_secs ELSE 0 END,0)) AS select_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'CURSOR' THEN query_count ELSE 0 END,0))::bigint AS cursor_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'CURSOR' THEN query_duration_secs ELSE 0 END,0)) AS cursor_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'UNLOAD' THEN query_count ELSE 0 END,0))::bigint AS unload_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'UNLOAD' THEN query_duration_secs ELSE 0 END,0)) AS unload_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'COPY' THEN query_count ELSE 0 END,0))::bigint AS copy_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'COPY' THEN query_duration_secs ELSE 0 END,0)) AS copy_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'INSERT' THEN query_count ELSE 0 END,0))::bigint AS insert_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'INSERT' THEN query_duration_secs ELSE 0 END,0)) AS insert_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'UPDATE' THEN query_count ELSE 0 END,0))::bigint AS update_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'UPDATE' THEN query_duration_secs ELSE 0 END,0)) AS update_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'DELETE' THEN query_count ELSE 0 END,0))::bigint AS delete_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'DELETE' THEN query_duration_secs ELSE 0 END,0)) AS delete_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'VACUUM' THEN query_count ELSE 0 END,0))::bigint AS vacuum_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'VACUUM' THEN query_duration_secs ELSE 0 END,0)) AS vacuum_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'ANALYZE' THEN query_count ELSE 0 END,0))::bigint AS analyze_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'ANALYZE' THEN query_duration_secs ELSE 0 END,0)) AS analyze_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'CALL_PROCEDURE' THEN query_count ELSE 0 END,0))::bigint AS call_proc_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'CALL_PROCEDURE' THEN query_duration_secs ELSE 0 END,0)) AS call_proc_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'DDL' THEN query_count ELSE 0 END,0))::bigint AS ddl_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'DDL' THEN query_duration_secs ELSE 0 END,0)) AS ddl_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'DCL' THEN query_count ELSE 0 END,0))::bigint AS dcl_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'DCL' THEN query_duration_secs ELSE 0 END,0)) AS dcl_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'AUTOMV' THEN query_count ELSE 0 END,0))::bigint AS automv_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'AUTOMV' THEN query_duration_secs ELSE 0 END,0)) AS automv_duration_secs,\n       MAX(NVL(CASE WHEN wp.query_type = 'OTHER' THEN query_count ELSE 0 END,0))::bigint AS other_count,\n       MAX(NVL(CASE WHEN wp.query_type = 'OTHER' THEN query_duration_secs ELSE 0 END,0)) AS other_duration_secs\nFROM workload_profile wp\nWHERE wp.dbname <> 'padb_harvest'\nGROUP BY 1,2,3\nORDER BY 1,2,3;",
            "Signals": [
                {
                    "Signal": "high execution count of COPY command",
                    "Criteria": "copy_count > 100",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "23"
                    ]
                },
                {
                    "Signal": "high execution count of single row insert statements",
                    "Criteria": "insert_count > 100",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "29"
                    ]
                },
                {
                    "Signal": "high execution count of DDL command",
                    "Criteria": "ddl_count > 10",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "25"
                    ]
                }
            ],
            "Observations": []
        },
        "ConcurrencyScalingUsage": {
            "SQL": "select date_trunc('hour',end_time) as burst_hour\n      ,sum(queries) as query_count\n      ,sum(usage_in_seconds) as burst_usage_in_seconds  \nfrom svcs_concurrency_scaling_usage\ngroup by burst_hour;",
            "Signals": [
                {
                    "Signal": "concurrency scaling use is more than 1 hour per day",
                    "Criteria": "burst_usage_in_seconds > 3600",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "33"
                    ]
                }
            ],
            "Observations": []
        },
        "TableAlerts": {
            "SQL": "SELECT trim(q.\"database\") as dbname,\n       trim(s.perm_table_name) AS table_name,\n       COALESCE(SUM(ABS(datediff (microsecond,COALESCE(b.starttime,d.starttime,s.starttime),CASE WHEN COALESCE(b.endtime,d.endtime,s.endtime) >COALESCE(b.starttime,d.starttime,s.starttime) THEN COALESCE(b.endtime,d.endtime,s.endtime) ELSE COALESCE(b.starttime,d.starttime,s.starttime) END)))/ 1000000::NUMERIC(38,4),0) AS alert_seconds,\n       COALESCE(SUM(COALESCE(b.rows,d.rows,s.rows)),0) AS alert_rowcount,\n       trim(split_part (l.event,':',1)) AS alert_event,\n       substring(trim(l.solution),1,200) AS alert_solution,\n       MAX(l.query) AS alert_sample_query,\n       COUNT(DISTINCT l.query) alert_querycount\nFROM stl_alert_event_log AS l\n  LEFT JOIN stl_scan AS s\n         ON s.query = l.query\n        AND s.slice = l.slice\n        AND s.segment = l.segment\n        AND s.userid > 1\n        AND s.perm_table_name NOT IN ('Internal Worktable','S3')\n\t\tAND s.perm_table_name NOT LIKE ('volt_tt%')\n\t\tAND s.perm_table_name NOT LIKE ('mv_tbl__auto_mv%')\t\t\t\n  LEFT JOIN stl_dist AS d\n         ON d.query = l.query\n        AND d.slice = l.slice\n        AND d.segment = l.segment\n        AND d.userid > 1\n  LEFT JOIN stl_bcast AS b\n         ON b.query = l.query\n        AND b.slice = l.slice\n        AND b.segment = l.segment\n        AND b.userid > 1    \n  LEFT JOIN stl_query AS q   \n         ON q.query = l.query\n        AND q.xid = l.xid\n        AND q.userid > 1           \nWHERE l.userid > 1\n  AND trim(s.perm_table_name) IS NOT NULL\n  AND l.event_time >= dateadd(day,- 7,CURRENT_DATE)\nGROUP BY 1,2,5,6\nORDER BY alert_seconds DESC;",
            "Signals": [
                {
                    "Signal": "tables w/ high # of distribution alerts",
                    "Criteria": "alert_querycount > 5 and alert_solution.str.contains('distribution')",
                    "PopulationCriteria": "table_name != '' and alert_seconds > 300 and alert_querycount> 10",
                    "Recommendation": [
                        "8"
                    ]
                },
                {
                    "Signal": "tables w/ high # of sort key alerts",
                    "Criteria": "alert_querycount > 5 and alert_solution.str.contains('sort key') ",
                    "PopulationCriteria": "table_name != '' and alert_seconds > 300 and alert_querycount > 10",
                    "Recommendation": [
                        "7"
                    ]
                },
                {
                    "Signal": "alerts regarding missing statistics",
                    "Criteria": "alert_querycount > 5 and alert_solution.str.contains('ANALYZE')",
                    "PopulationCriteria": "alert_querycount > 10",
                    "Recommendation": [
                        "3"
                    ]
                },
                {
                    "Signal": "alerts regarding nested loops",
                    "Criteria": "alert_querycount > 5 and alert_solution.str.contains('Cartesian')",
                    "PopulationCriteria": "alert_querycount > 10",
                    "Recommendation": [
                        "9",
                        "22"
                    ]
                }
            ],
            "Observations": []
        },
        "TableInfo": {
            "SQL": "SELECT t.\"database\" AS dbname,\n       t.\"schema\" AS namespace,\n       t.\"table\" table_name,\n       t.encoded,\n       t.diststyle,\n       t.sortkey1,\n       t.max_varchar,\n       trim(t.sortkey1_enc) AS sortkey1_enc,\n       t.sortkey_num,\n       t.unsorted,\n       t.stats_off,\n       t.tbl_rows,\n       t.skew_rows,\n       t.estimated_visible_rows,\n       CASE\n         WHEN t.tbl_rows - t.estimated_visible_rows < 0 THEN 0\n         ELSE (t.tbl_rows - t.estimated_visible_rows)\n       END AS num_rows_marked_for_deletion,\n       CASE\n         WHEN t.tbl_rows - t.estimated_visible_rows < 0 THEN 0\n         ELSE (t.tbl_rows - t.estimated_visible_rows) /\n           CASE\n             WHEN nvl (t.tbl_rows,0) = 0 THEN 1\n             ELSE t.tbl_rows\n           END ::NUMERIC(38,4)\n       END AS pct_rows_marked_for_deletion,\n       t.vacuum_sort_benefit,\n       v.vacuum_run_type,\n       v.is_last_vacuum_recluster,\n       v.last_vacuumed_date,\n       v.days_since_last_vacuumed,\n       NVL(s.num_qs,0) query_count,\n       nvl(sat.table_recommendation_count,0) AS table_recommendation_count,\n       c.encoded_column_count,\n       c.column_count,\n       c.encoded_column_pct::NUMERIC(38,4) AS encoded_column_pct,\n       c.encoded_sortkey_count,\n       c.distkey_column_count,\n       nvl(tc.large_column_size_count,0) AS large_column_size_count,\n       tak.alert_sample_query AS sort_key_alert_sample_query,\n       nvl(tak.alert_query_count,0) AS sort_key_alert_query_count,\n       tas.alert_sample_query AS stats_alert_sample_query,\n       nvl(tas.alert_query_count,0) AS stats_alert_query_count,\n       tanl.alert_sample_query AS nl_alert_sample_query,\n       nvl(tanl.alert_query_count,0) AS nl_alert_query_count,\n       tad.alert_sample_query AS distributed_alert_sample_query,\n       nvl(tad.alert_query_count,0) AS distributed_alert_query_count,\n       tab.alert_sample_query AS distributed_alert_sample_query,\n       nvl(tab.alert_query_count,0) AS broadcasted_alert_query_count,\n       tax.alert_sample_query AS deleted_alert_sample_query,\n       nvl(tax.alert_query_count,0) AS deleted_alert_query_count\nFROM SVV_TABLE_INFO t\n  INNER JOIN (SELECT attrelid,\n                     COUNT(1) column_count,\n                     SUM(CASE WHEN attisdistkey = FALSE THEN 0 ELSE 1 END) AS distkey_column_count,\n                     SUM(CASE WHEN attencodingtype IN (0,128) THEN 0 ELSE 1 END) AS encoded_column_count,\n                     1.0 *SUM(CASE WHEN attencodingtype IN (0,128) THEN 0 ELSE 1 END) / COUNT(1)*100 encoded_column_pct,\n                     SUM(CASE WHEN attencodingtype NOT IN (0,128) AND attsortkeyord > 0 THEN 1 ELSE 0 END) AS encoded_sortkey_count\n              FROM pg_attribute\n              WHERE attnum > 0\n              GROUP BY attrelid) c ON (c.attrelid = t.table_id)\n  LEFT OUTER JOIN (SELECT tbl,\n                          perm_table_name,\n                          COUNT(DISTINCT query) num_qs\n                   FROM stl_scan s\n                   WHERE s.userid > 1\n                   AND   s.perm_table_name NOT IN ('Internal Worktable','S3')\n                   GROUP BY 1,\n                            2) s ON (s.tbl = t.table_id)\n  LEFT OUTER JOIN (SELECT DATABASE,\n                          table_id,\n                          COUNT(1) AS table_recommendation_count\n                   FROM svv_alter_table_recommendations\n                   GROUP BY DATABASE,\n                            table_id) sat\n               ON (sat.table_id = t.table_id\n              AND sat.database = t.database)\n  LEFT OUTER JOIN (SELECT sc.table_catalog AS database_name,\n                          sc.table_schema,\n                          sc.table_name,\n                          SUM(CASE WHEN sc.character_maximum_length > 1000 THEN 1 ELSE 0 END) AS large_column_size_count\n                   FROM svv_columns sc\n                     INNER JOIN svv_table_info st\n                             ON (sc.table_catalog = st.database\n                            AND sc.table_schema = st.schema\n                            AND sc.table_name = st.table)\n                   WHERE sc.data_type IN ('character varying','character')\n                   AND   sc.character_maximum_length > 1000\n                   AND   sc.table_schema NOT IN ('pg_internal','pg_catalog','pg_automv')\n                   GROUP BY sc.table_catalog,\n                            sc.table_schema,\n                            sc.table_name) tc\n               ON (t.database = tc.database_name\n              AND tc.table_name = t.table\n              AND tc.table_schema = t.schema)\n  LEFT OUTER JOIN (SELECT t.database AS database_name,\n                          t.schema AS schema_name,\n                          t.table AS table_name,\n                          v.table_id,\n                          CASE\n                            WHEN v.status LIKE '%VacuumBG%' THEN 'Automatic'\n                            ELSE 'Manual'\n                          END AS vacuum_run_type,\n                          CASE\n                            WHEN v.is_recluster = 0 THEN 'N'\n                            WHEN v.is_recluster = 1 THEN 'Y'\n                            ELSE NULL\n                          END AS is_last_vacuum_recluster,\n                          CAST(MAX(v.eventtime) AS DATE) AS last_vacuumed_date,\n                          datediff(d,CAST(MAX(v.eventtime) AS DATE),CAST(CURRENT_TIMESTAMP AT TIME ZONE 'UTC' AS DATE)) AS days_since_last_vacuumed\n                   FROM stl_vacuum v\n                     INNER JOIN svv_table_info t ON (t.table_id = v.table_id)\n                   WHERE v.status NOT LIKE '%Started%'\n                   GROUP BY 1,\n                            2,\n                            3,\n                            4,\n                            5,\n                            6) v\n               ON (v.database_name = t.database\n              AND v.table_name = t.table\n              AND v.schema_name = t.schema)\n  LEFT OUTER JOIN (SELECT q.database AS database_name,\n                          TRIM(s.perm_table_name) AS table_name,\n                          MAX(l.query) AS alert_sample_query,\n                          COUNT(DISTINCT l.query) AS alert_query_count\n                   FROM stl_alert_event_log AS l\n                     LEFT JOIN stl_scan AS s\n                            ON s.query = l.query\n                           AND s.slice = l.slice\n                           AND s.segment = l.segment\n                           AND s.userid > 1\n                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n                     LEFT JOIN stl_query AS q\n                            ON q.query = l.query\n                           AND q.xid = l.xid\n                           AND q.userid > 1\n                   WHERE l.userid > 1\n                   AND   TRIM(s.perm_table_name) IS NOT NULL\n                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n                   AND   TRIM(split_part(l.event,':',1)) = 'Very selective query filter'\n                   GROUP BY 1,\n                            2) tak\n               ON (tak.database_name = t.database\n              AND tak.table_name = t.table)\n  LEFT OUTER JOIN (SELECT q.database AS database_name,\n                          TRIM(s.perm_table_name) AS table_name,\n                          MAX(l.query) AS alert_sample_query,\n                          COUNT(DISTINCT l.query) AS alert_query_count\n                   FROM stl_alert_event_log AS l\n                     LEFT JOIN stl_scan AS s\n                            ON s.query = l.query\n                           AND s.slice = l.slice\n                           AND s.segment = l.segment\n                           AND s.userid > 1\n                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n                     LEFT JOIN stl_query AS q\n                            ON q.query = l.query\n                           AND q.xid = l.xid\n                           AND q.userid > 1\n                   WHERE l.userid > 1\n                   AND   TRIM(s.perm_table_name) IS NOT NULL\n                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n                   AND   TRIM(split_part(l.event,':',1)) = 'Missing query planner statistics'\n                   GROUP BY 1,\n                            2) tas\n               ON (tas.database_name = t.database\n              AND tas.table_name = t.table)\n  LEFT OUTER JOIN (SELECT q.database AS database_name,\n                          TRIM(s.perm_table_name) AS table_name,\n                          MAX(l.query) AS alert_sample_query,\n                          COUNT(DISTINCT l.query) AS alert_query_count\n                   FROM stl_alert_event_log AS l\n                     LEFT JOIN stl_scan AS s\n                            ON s.query = l.query\n                           AND s.slice = l.slice\n                           AND s.segment = l.segment\n                           AND s.userid > 1\n                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n                     LEFT JOIN stl_query AS q\n                            ON q.query = l.query\n                           AND q.xid = l.xid\n                           AND q.userid > 1\n                   WHERE l.userid > 1\n                   AND   TRIM(s.perm_table_name) IS NOT NULL\n                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n                   AND   TRIM(split_part(l.event,':',1)) = 'Nested Loop Join in the query plan'\n                   GROUP BY 1,\n                            2) tanl\n               ON (tanl.database_name = t.database\n              AND tanl.table_name = t.table)\n  LEFT OUTER JOIN (SELECT q.database AS database_name,\n                          TRIM(s.perm_table_name) AS table_name,\n                          MAX(l.query) AS alert_sample_query,\n                          COUNT(DISTINCT l.query) AS alert_query_count\n                   FROM stl_alert_event_log AS l\n                     LEFT JOIN stl_scan AS s\n                            ON s.query = l.query\n                           AND s.slice = l.slice\n                           AND s.segment = l.segment\n                           AND s.userid > 1\n                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n                     LEFT JOIN stl_query AS q\n                            ON q.query = l.query\n                           AND q.xid = l.xid\n                           AND q.userid > 1\n                   WHERE l.userid > 1\n                   AND   TRIM(s.perm_table_name) IS NOT NULL\n                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n                   AND   TRIM(split_part(l.event,':',1)) = 'Distributed a large number of rows across the network'\n                   GROUP BY 1,\n                            2) tad\n               ON (tad.database_name = t.database\n              AND tad.table_name = t.table)\n  LEFT OUTER JOIN (SELECT q.database AS database_name,\n                          TRIM(s.perm_table_name) AS table_name,\n                          MAX(l.query) AS alert_sample_query,\n                          COUNT(DISTINCT l.query) AS alert_query_count\n                   FROM stl_alert_event_log AS l\n                     LEFT JOIN stl_scan AS s\n                            ON s.query = l.query\n                           AND s.slice = l.slice\n                           AND s.segment = l.segment\n                           AND s.userid > 1\n                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n                     LEFT JOIN stl_query AS q\n                            ON q.query = l.query\n                           AND q.xid = l.xid\n                           AND q.userid > 1\n                   WHERE l.userid > 1\n                   AND   TRIM(s.perm_table_name) IS NOT NULL\n                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n                   AND   TRIM(split_part(l.event,':',1)) = 'Broadcasted a large number of rows across the network'\n                   GROUP BY 1,\n                            2) tab\n               ON (tab.database_name = t.database\n              AND tab.table_name = t.table)\n  LEFT OUTER JOIN (SELECT q.database AS database_name,\n                          TRIM(s.perm_table_name) AS table_name,\n                          MAX(l.query) AS alert_sample_query,\n                          COUNT(DISTINCT l.query) AS alert_query_count\n                   FROM stl_alert_event_log AS l\n                     LEFT JOIN stl_scan AS s\n                            ON s.query = l.query\n                           AND s.slice = l.slice\n                           AND s.segment = l.segment\n                           AND s.userid > 1\n                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n                     LEFT JOIN stl_query AS q\n                            ON q.query = l.query\n                           AND q.xid = l.xid\n                           AND q.userid > 1\n                   WHERE l.userid > 1\n                   AND   TRIM(s.perm_table_name) IS NOT NULL\n                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n                   AND   TRIM(split_part(l.event,':',1)) = 'Scanned a large number of deleted rows'\n                   GROUP BY 1,\n                            2) tax\n               ON (tax.database_name = t.database\n              AND tax.table_name = t.table)\nWHERE t.\"schema\" NOT IN ('pg_internal','pg_catalog','pg_automv')\nAND   t.\"schema\" NOT LIKE 'pg_temp%'\nORDER BY tbl_rows DESC;",
            "Signals": [
                {
                    "Signal": "tables with wide columns",
                    "Criteria": "max_varchar > 1000",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "11"
                    ]
                },
                {
                    "Signal": "large tables without a sort key",
                    "Criteria": "sortkey1 == ''",
                    "PopulationCriteria": "tbl_rows > 5000000",
                    "Recommendation": [
                        "7"
                    ]
                },
                {
                    "Signal": "large tables with skew",
                    "Criteria": "skew_rows >= 4 and not diststyle.str.startswith('AUTO')",
                    "PopulationCriteria": "tbl_rows > 5000000",
                    "Recommendation": [
                        "8"
                    ]
                },
                {
                    "Signal": "large tables with unsorted data",
                    "Criteria": "vacuum_sort_benefit >= 10",
                    "PopulationCriteria": "tbl_rows > 5000000",
                    "Recommendation": [
                        "13"
                    ]
                },
                {
                    "Signal": "tables with interleaved sort keys",
                    "Criteria": "sortkey1.str.contains('INTERLEAVED')",
                    "PopulationCriteria": "tbl_rows > 5000000",
                    "Recommendation": [
                        "14"
                    ]
                },
                {
                    "Signal": "small tables without an ALL distribution",
                    "Criteria": "not diststyle.str.contains('ALL')",
                    "PopulationCriteria": "tbl_rows <= 5000000",
                    "Recommendation": [
                        "8"
                    ]
                },
                {
                    "Signal": "small tables with a sort key",
                    "Criteria": "sortkey1 != ''",
                    "PopulationCriteria": "tbl_rows <= 5000000",
                    "Recommendation": [
                        "7"
                    ]
                },
                {
                    "Signal": "large tables needing a vacuum delete",
                    "Criteria": "pct_rows_marked_for_deletion > 10",
                    "PopulationCriteria": "tbl_rows > 5000000",
                    "Recommendation": [
                        "2"
                    ]
                },
                {
                    "Signal": "tables with out of date statistics",
                    "Criteria": "stats_off > 10",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "3"
                    ]
                },
                {
                    "Signal": "tables with encoded sort keys",
                    "Criteria": "sortkey1_enc != 'none' and sortkey1_enc != ''",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "10"
                    ]
                },
                {
                    "Signal": "tables with low column compression",
                    "Criteria": "encoded_column_pct < 80",
                    "PopulationCriteria": "(column_count - encoded_column_count) - sortkey_num > 1 AND tbl_rows > 500000",
                    "Recommendation": [
                        "4"
                    ]
                }
            ],
            "Observations": []
        },
        "AlterTableRecommendations": {
            "SQL": "SELECT r.type,\n       trim(t.database_name) AS dbname,\n       t.schema_name AS namespace,\n       r.table_id,\n       t.table_name,\n       r.group_id,\n       r.ddl,\n       r.auto_eligible\nFROM svv_alter_table_recommendations r\n  INNER JOIN (SELECT DISTINCT(stv_tbl_perm.id) AS table_id\n                    ,TRIM(pg_database.datname) AS database_name\n                    ,TRIM(pg_namespace.nspname) AS schema_name\n                    ,TRIM(relname) AS table_name\n                FROM stv_tbl_perm\n              INNER JOIN pg_database on pg_database.oid = stv_tbl_perm.db_id\n              INNER JOIN pg_class on pg_class.oid = stv_tbl_perm.id\n              INNER JOIN pg_namespace on pg_namespace.oid = pg_class.relnamespace\n               WHERE schema_name NOT IN ('pg_internal', 'pg_catalog','pg_automv')) t\n          ON (r.database = t.database_name\n         AND r.table_id = t.table_id);",
            "Signals": [
                {
                    "Signal": "column encoding recommendation on table is not automatically applied",
                    "Criteria": "type=='encode' and auto_eligible=='f'",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "4"
                    ]
                },
                {
                    "Signal": "sort key recommendation on table is not automatically applied",
                    "Criteria": "type=='sortkey' and auto_eligible=='f'",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "7"
                    ]
                },
                {
                    "Signal": "dist key recommendation on table is not automatically applied",
                    "Criteria": "type=='diststyle' and auto_eligible=='f'",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "8"
                    ]
                }
            ],
            "Observations": []
        },
        "MaterializedView": {
            "SQL": "WITH mv_info AS\n(\n  SELECT trim(db_name) AS dbname,\n         trim(\"schema\") AS namespace, \n         trim(name) AS mview_name,\n         is_stale,\n         state,\n         CASE state\n           WHEN 0 THEN 'The MV is fully recomputed when refreshed'\n           WHEN 1 THEN 'The MV is incremental'\n           WHEN 101 THEN 'The MV cant be refreshed due to a dropped column. This constraint applies even if the column isnt used in the MV'\n           WHEN 102 THEN 'The MV cant be refreshed due to a changed column type. This constraint applies even if the column isnt used in the MV'\n           WHEN 103 THEN 'The MV cant be refreshed due to a renamed table'\n           WHEN 104 THEN 'The MV cant be refreshed due to a renamed column. This constraint applies even if the column isnt used in the MV'\n           WHEN 105 THEN 'The MV cant be refreshed due to a renamed schema'\n           ELSE NULL\n         END AS state_desc,\n         autorewrite,\n         autorefresh\n  FROM stv_mv_info\n),\nmv_state AS\n(\n  SELECT dbname,\n         namespace,\n         mview_name,\n         state AS mv_state,\n         event_desc,\n         starttime AS event_starttime\n  FROM (SELECT trim(db_name) AS dbname,\n               trim(mv_schema) AS namespace, \n               trim(mv_name) AS mview_name,\n               ROW_NUMBER() OVER (PARTITION BY db_name, mv_schema, mv_name ORDER BY starttime DESC) AS rnum,\n               state,\n               event_desc,\n               starttime\n        FROM stl_mv_state)\n  WHERE rnum = 1\n),\nmv_ref_status AS\n(\n  SELECT dbname,\n         refresh_db_username,\n         namespace,\n         mview_name,\n         status AS refresh_status,\n         refresh_type,\n         starttime AS refresh_starttime,\n         endtime AS refresh_endtime,\n         datediff(ms, starttime, endtime)*1.0/1000 AS refresh_duration_secs\n  FROM (SELECT trim(r.db_name) AS dbname,\n               trim(r.schema_name) AS namespace, \n               pu.usename as refresh_db_username,\n               trim(r.mv_name) AS mview_name,\n               ROW_NUMBER() OVER (PARTITION BY r.db_name, r.schema_name, r.userid, r.mv_name ORDER BY starttime DESC) AS rnum,\n               r.status,\n               r.refresh_type,\n               r.starttime,\n               r.endtime\n        FROM svl_mv_refresh_status r\n        INNER JOIN pg_user pu on (r.userid = pu.usesysid))\n  WHERE rnum = 1\n)\nSELECT i.*, \n       s.mv_state, s.event_desc, s.event_starttime,\n       r.refresh_db_username, r.refresh_status, r.refresh_type, r.refresh_starttime, r.refresh_endtime, r.refresh_duration_secs\nFROM mv_info i\n  LEFT JOIN mv_state s ON (i.dbname = s.dbname AND i.namespace = s.namespace AND i.mview_name = s.mview_name)\n  LEFT JOIN mv_ref_status r ON (i.dbname = r.dbname AND i.namespace = r.namespace AND i.mview_name = r.mview_name);",
            "Signals": [
                {
                    "Signal": "materialized view is doing a full refresh",
                    "Criteria": "state==0",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "30"
                    ]
                },
                {
                    "Signal": "materialized view cannot be refreshed",
                    "Criteria": "state > 1 and is_stale == 't'",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "31"
                    ]
                }
            ],
            "Observations": []
        },
        "Top50QueriesByRunTime": {
            "SQL": "SELECT TRIM(dbname) AS dbname,\n       TRIM(db_username) AS db_username,\n       MAX(SUBSTRING(replace(qrytext,chr (34),chr (92) + chr (34)),1,500)) AS qrytext,\n       COUNT(query) AS num_queries,\n       MIN(run_minutes) AS min_minutes,\n       MAX(run_minutes) AS max_minutes,\n       AVG(run_minutes) AS avg_minutes,\n       SUM(run_minutes) AS total_minutes,\n       SUM(compile_minutes) AS total_compile_minutes,\n       SUM(num_compile_segments) AS total_num_compile_segments,\n       MIN(query_temp_blocks_to_disk_mb) AS min_disk_spill_mb,\n       MAX(query_temp_blocks_to_disk_mb) AS max_disk_spill_mb,\n       AVG(query_temp_blocks_to_disk_mb) AS avg_disk_spill_mb,\n       SUM(query_temp_blocks_to_disk_mb) AS total_disk_spill_mb,\n       MAX(query) AS max_query_id,\n       MAX(starttime)::DATE AS last_run,\n       COUNT(DISTINCT starttime::DATE) AS num_days_executed,\n       SUM(aborted) AS total_aborted,\n       MAX(mylabel) qry_label,\n       AVG(spectrum_object_count) AS avg_spectrum_object_used,\n       AVG(federated_object_count) AS avg_federated_object_used,\n       user_table_involved,\n       TRIM(DECODE (event & 1,1,'Sortkey ','') || DECODE (event & 2,2,'Deletes ','') || DECODE (event & 4,4,'NL ','') || DECODE (event & 8,8,'Dist ','') || DECODE (event & 16,16,'Broacast ','') || DECODE (event & 32,32,'Stats ','')) AS Alert\nFROM (SELECT stl_query.userid,\n             pu.usename as db_username,\n             label,\n             stl_query.query,\n             TRIM(\"DATABASE\") AS dbname,\n             NVL(qrytext_cur.text,TRIM(querytxt)) AS qrytext,\n             MD5(NVL (qrytext_cur.text,TRIM(querytxt))) AS qry_md5,\n             starttime,\n             endtime,\n             DATEDIFF(seconds,starttime,endtime)::NUMERIC(12,2) / 60 AS run_minutes,\n             aborted,\n             event,\n             stl_query.label AS mylabel,\n             CASE\n               WHEN sqms.query_temp_blocks_to_disk IS NULL THEN 0\n               ELSE sqms.query_temp_blocks_to_disk\n             END AS query_temp_blocks_to_disk_mb,\n             nvl(compile_secs,0)::NUMERIC(12,2) / 60 AS compile_minutes,\n             nvl(num_compile_segments,0) AS num_compile_segments,\n             s.user_table_involved,\n             NVL(s3.spectrum_object_count,0) AS spectrum_object_count,\n             NVL(f.federated_object_count,0) AS federated_object_count\n      FROM stl_query\n\t  INNER JOIN pg_catalog.pg_user pu on (stl_query.userid = pu.usesysid)\n        LEFT OUTER JOIN (SELECT query,\n                                SUM(DECODE (TRIM(SPLIT_PART (event,':',1)),'Very selective query filter',1,'Scanned a large number of deleted rows',2,'Nested Loop Join in the query plan',4,'Distributed a large number of rows across the network',8,'Broadcasted a large number of rows across the network',16,'Missing query planner statistics',32,0)) AS event\n                         FROM stl_alert_event_log\n                         WHERE event_time >= DATEADD(DAY,-7,CURRENT_DATE)\n                         GROUP BY query) AS alrt ON alrt.query = stl_query.query\n        LEFT OUTER JOIN (SELECT ut.xid,\n                                TRIM(SUBSTRING(text FROM STRPOS (UPPER(text),'SELECT'))) AS TEXT\n                         FROM stl_utilitytext ut\n                         WHERE SEQUENCE = 0\n                         AND   text ilike 'DECLARE%'\n                         GROUP BY text,\n                                  ut.xid) qrytext_cur ON (stl_query.xid = qrytext_cur.xid)\n        LEFT OUTER JOIN svl_query_metrics_summary sqms\n                     ON (sqms.userid = stl_query.userid\n                    AND sqms.query = stl_query.query)\n        LEFT OUTER JOIN (SELECT userid,\n                                xid,\n                                pid,\n                                query,\n                                MAX(datediff (ms,starttime,endtime)*1.0 / 1000) AS compile_secs,\n                                SUM(compile) AS num_compile_segments\n                         FROM svcs_compile\n                         GROUP BY userid,\n                                  xid,\n                                  pid,\n                                  query) c\n                     ON (c.userid = stl_query.userid\n                    AND c.xid = stl_query.xid\n                    AND c.pid = stl_query.pid\n                    AND c.query = stl_query.query)\n        LEFT OUTER JOIN (SELECT s.userid,\n                                s.query,\n                                LISTAGG(DISTINCT TRIM(s.perm_table_name),', ') AS user_table_involved\n                         FROM stl_scan s\n                             INNER JOIN (SELECT DISTINCT(stv_tbl_perm.id) AS table_id\n                    ,TRIM(pg_database.datname) AS database_name\n                    ,TRIM(pg_namespace.nspname) AS schema_name\n                    ,TRIM(relname) AS table_name\n                FROM stv_tbl_perm\n              INNER JOIN pg_database on pg_database.oid = stv_tbl_perm.db_id\n              INNER JOIN pg_class on pg_class.oid = stv_tbl_perm.id\n              INNER JOIN pg_namespace on pg_namespace.oid = pg_class.relnamespace\n               WHERE schema_name NOT IN ('pg_internal', 'pg_catalog','pg_automv')) t ON (s.tbl = t.table_id)\n                         WHERE s.perm_table_name NOT IN ('Internal Worktable','S3')\n                         AND   s.perm_table_name NOT LIKE ('volt_tt%')\n                         AND   t.schema_name NOT IN ('pg_internal','pg_catalog')\n                         GROUP BY s.userid,\n                                  s.query) s\n                     ON (stl_query.userid = s.userid\n                    AND stl_query.query = s.query)\n        LEFT OUTER JOIN (SELECT s.userid,\n                                s.query,\n                                COUNT(1) AS spectrum_object_count\n                         FROM svl_s3query_summary s\n                         WHERE s.external_table_name NOT IN ('PG Subquery')\n                         GROUP BY s.userid,\n                                  s.query) s3\n                     ON (stl_query.userid = s3.userid\n                    AND stl_query.query = s3.query)\n        LEFT OUTER JOIN (SELECT f.userid,\n                                f.query,\n                                COUNT(1) AS federated_object_count\n                         FROM svl_federated_query f\n                         GROUP BY f.userid,\n                                  f.query) f\n                     ON (stl_query.userid = f.userid\n                    AND stl_query.query = f.query)\n      WHERE stl_query.userid <> 1\n      AND   NVL(qrytext_cur.text,TRIM(querytxt)) NOT LIKE 'padb_fetch_sample:%'\n      AND   NVL(qrytext_cur.text,TRIM(querytxt)) NOT LIKE 'CREATE TEMP TABLE volt_tt_%'\n      AND   stl_query.starttime >= DATEADD(DAY,-7,CURRENT_DATE))\nGROUP BY TRIM(dbname),\n         TRIM(db_username),\n         qry_md5,\n         user_table_involved,\n         event\nORDER BY avg_minutes DESC, num_queries DESC LIMIT 50;",
            "Signals": [
                {
                    "Signal": "long running queries with missing Table Statistics",
                    "Criteria": "alert.str.contains('Stats') and not qrytext.str.contains('fetch_sample')",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "3"
                    ]
                },
                {
                    "Signal": "long running queries using Nested Loop Joins",
                    "Criteria": "alert.str.contains('NL')",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "22",
                        "9"
                    ]
                },
                {
                    "Signal": "long running queries with Dist or Broadcast alerts",
                    "Criteria": "alert.str.contains('Dist') or alert.str.contains('Broacast') and not qrytext.str.contains('fetch_sample')",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "8"
                    ]
                },
                {
                    "Signal": "long running queries with Sort alerts",
                    "Criteria": "alert.str.contains('Sort')",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "7"
                    ]
                }
            ],
            "Observations": []
        },
        "Top50QueriesByDiskSpill": {
            "SQL": "SELECT TRIM(dbname) AS dbname,\n       TRIM(db_username) AS db_username,\n       MAX(SUBSTRING(replace(qrytext,chr (34),chr (92) + chr (34)),1,500)) AS qrytext,\n       COUNT(query) AS num_queries,\t    \n       MIN(run_minutes) AS min_minutes,\n       MAX(run_minutes) AS max_minutes,\n       AVG(run_minutes) AS avg_minutes,\n       SUM(run_minutes) AS total_minutes,\n       SUM(compile_minutes) AS total_compile_minutes,\n       SUM(num_compile_segments) AS total_num_compile_segments,\n       MIN(query_temp_blocks_to_disk_mb) AS min_disk_spill_mb,\n       MAX(query_temp_blocks_to_disk_mb) AS max_disk_spill_mb,\n       AVG(query_temp_blocks_to_disk_mb) AS avg_disk_spill_mb,\n       SUM(query_temp_blocks_to_disk_mb) AS total_disk_spill_mb,\n       MAX(query) AS max_query_id,\n       MAX(starttime)::DATE AS last_run,\n       COUNT(DISTINCT starttime::DATE) AS num_days_executed,\t\t   \n       SUM(aborted) as total_aborted,\n       MAX(mylabel) qry_label,\n\t   AVG(spectrum_object_count) AS avg_spectrum_object_used,\n\t   AVG(federated_object_count) AS avg_federated_object_used,\n\t   user_table_involved,\n       TRIM(DECODE (event & 1,1,'Sortkey ','') || DECODE (event & 2,2,'Deletes ','') || DECODE (event & 4,4,'NL ','') || DECODE (event & 8,8,'Dist ','') || DECODE (event & 16,16,'Broacast ','') || DECODE (event & 32,32,'Stats ','')) AS Alert\nFROM (SELECT stl_query.userid,\n             pu.usename as db_username,\n             label,\n             stl_query.query,\n             TRIM(\"DATABASE\") AS dbname,\n             NVL(qrytext_cur.text,TRIM(querytxt)) AS qrytext,\n             MD5(NVL (qrytext_cur.text,TRIM(querytxt))) AS qry_md5,\n             starttime,\n             endtime,\n             DATEDIFF(seconds,starttime,endtime)::NUMERIC(12,2) / 60 AS run_minutes,\n             aborted,\n             event,\n             stl_query.label AS mylabel,\n             CASE\n               WHEN sqms.query_temp_blocks_to_disk IS NULL THEN 0\n               ELSE sqms.query_temp_blocks_to_disk\n             END AS query_temp_blocks_to_disk_mb,\n             nvl(compile_secs,0)::NUMERIC(12,2) / 60 AS compile_minutes,\n             nvl(num_compile_segments,0) AS num_compile_segments,\n\t\t\t s.user_table_involved,\n\t\t\t NVL(s3.spectrum_object_count,0) AS spectrum_object_count,\n\t\t\t NVL(f.federated_object_count,0) AS federated_object_count\n        FROM stl_query\n\t\tINNER JOIN pg_catalog.pg_user pu on (stl_query.userid = pu.usesysid)\n        LEFT OUTER JOIN (SELECT query,\n                                SUM(DECODE (TRIM(SPLIT_PART (event,':',1)),'Very selective query filter',1,'Scanned a large number of deleted rows',2,'Nested Loop Join in the query plan',4,'Distributed a large number of rows across the network',8,'Broadcasted a large number of rows across the network',16,'Missing query planner statistics',32,0)) AS event\n                         FROM stl_alert_event_log\n                         WHERE event_time >= DATEADD(DAY,-7,CURRENT_DATE)\n                         GROUP BY query) AS alrt ON alrt.query = stl_query.query\n        LEFT OUTER JOIN (SELECT ut.xid,\n                                TRIM(SUBSTRING(text FROM STRPOS (UPPER(text),'SELECT'))) AS TEXT\n                         FROM stl_utilitytext ut\n                         WHERE SEQUENCE = 0\n                         AND   text ilike 'DECLARE%'\n                         GROUP BY text,\n                                  ut.xid) qrytext_cur ON (stl_query.xid = qrytext_cur.xid)\n        LEFT OUTER JOIN svl_query_metrics_summary sqms\n                     ON (sqms.userid = stl_query.userid AND sqms.query = stl_query.query)\n        LEFT OUTER JOIN (SELECT userid,\n                                xid,\n                                pid,\n                                query,\n                                MAX(datediff (ms,starttime,endtime)*1.0 / 1000) AS compile_secs,\n                                SUM(compile) AS num_compile_segments\n                         FROM svcs_compile\n                         GROUP BY userid,\n                                  xid,\n                                  pid,\n                                  query) c\n                     ON (c.userid = stl_query.userid\n                    AND c.xid = stl_query.xid\n                    AND c.pid = stl_query.pid\n                    AND c.query = stl_query.query)\n        LEFT OUTER JOIN (SELECT s.userid,\n                                s.query,\n                                LISTAGG(distinct TRIM(s.perm_table_name),', ') AS user_table_involved\n                           FROM stl_scan s\n                             INNER JOIN (SELECT DISTINCT(stv_tbl_perm.id) AS table_id\n                    ,TRIM(pg_database.datname) AS database_name\n                    ,TRIM(pg_namespace.nspname) AS schema_name\n                    ,TRIM(relname) AS table_name\n                FROM stv_tbl_perm\n              INNER JOIN pg_database on pg_database.oid = stv_tbl_perm.db_id\n              INNER JOIN pg_class on pg_class.oid = stv_tbl_perm.id\n              INNER JOIN pg_namespace on pg_namespace.oid = pg_class.relnamespace\n               WHERE schema_name NOT IN ('pg_internal', 'pg_catalog','pg_automv')) t ON (s.tbl = t.table_id)\n                          WHERE s.perm_table_name NOT IN ('Internal Worktable','S3')\n\t\t\t\t\t\t    AND s.perm_table_name NOT LIKE ('volt_tt%')\n                            AND t.schema_name NOT IN ('pg_internal', 'pg_catalog') \n                         GROUP BY s.userid,\n                                  s.query) s ON (stl_query.userid = s.userid AND stl_query.query = s.query)\t\n        LEFT OUTER JOIN (SELECT s.userid,\n                                s.query,\n                                COUNT(1) AS spectrum_object_count \n                           FROM svl_s3query_summary s \n\t\t\t\t\t\t  WHERE s.external_table_name NOT IN ('PG Subquery')\n                         GROUP BY s.userid,\n                                  s.query) s3 ON (stl_query.userid = s3.userid AND stl_query.query = s3.query)\n        LEFT OUTER JOIN (SELECT f.userid,\n                                f.query,\n                                COUNT(1) AS federated_object_count \n                           FROM svl_federated_query f \n                         GROUP BY f.userid,\n                                  f.query) f ON (stl_query.userid = f.userid AND stl_query.query = f.query)\t\t\t\t\t\t\t\t  \n      WHERE stl_query.userid <> 1\n\t    AND NVL(qrytext_cur.text,TRIM(querytxt)) NOT LIKE 'padb_fetch_sample:%' \n\t\tAND NVL(qrytext_cur.text,TRIM(querytxt)) NOT LIKE 'CREATE TEMP TABLE volt_tt_%'\n        AND stl_query.starttime >= DATEADD(DAY,-7,CURRENT_DATE))\nGROUP BY TRIM(dbname),\n         TRIM(db_username),\n         qry_md5,\n\t\t user_table_involved,\n         event\nORDER BY avg_disk_spill_mb DESC, num_queries DESC LIMIT 50;",
            "Signals": [
                {
                    "Signal": "high count of queries with large disk spill",
                    "Criteria": "avg_disk_spill_mb > 100000",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "11",
                        "22",
                        "6",
                        "5",
                        "15",
                        "24",
                        "28"
                    ]
                }
            ],
            "Observations": []
        },
        "CopyPerformance": {
            "SQL": "SELECT a.endtime::DATE AS copy_date,\n       trim(d.region) AS aws_region,\n       trim(d.s3_bucket) AS s3_bucket,\n       trim(d.file_format) AS file_format,\n       trim(q.\"database\") as dbname,\n       a.tbl AS table_id,\n       trim(c.nspname) AS namespace,\n       trim(b.relname) AS table_name,\n       SUM(a.rows_inserted) AS rows_inserted,\n       SUM(d.distinct_files) AS files_scanned,\n       SUM(d.mb_scanned) AS mb_scanned,\n       (SUM(d.distinct_files)::NUMERIC(19,3) / COUNT(DISTINCT a.query)::NUMERIC(19,3))::NUMERIC(19,3) AS avg_files_per_copy,\n       (SUM(d.mb_scanned) / SUM(d.distinct_files)::NUMERIC(19,3))::NUMERIC(19,3) AS avg_file_size_mb,\n       MAX(d.files_compressed) AS files_compressed,\n       MAX(cluster_slice_count) AS cluster_slice_count,\n       AVG(d.used_slice_count) AS avg_used_slice_count,\n       COUNT(DISTINCT a.query) no_of_copy,\n       MAX(a.query) AS sample_query,\n       ROUND((SUM(d.mb_scanned)*1024 *1000000.0 / SUM(d.load_micro)),4) AS scan_rate_kbps,\n       ROUND((SUM(a.rows_inserted)*1000000.0 / SUM(a.insert_micro)),4) AS insert_rate_rows_per_second,\n       ROUND(SUM(d.copy_duration_micro)/1000000.0,4) AS total_copy_time_secs,\n       ROUND(AVG(d.copy_duration_micro)/1000000.0,4) AS avg_copy_time_secs,\n       ROUND(SUM(d.compression_micro)/1000000.0,4) AS total_compression_time_secs,\n       ROUND(AVG(d.compression_micro)/1000000.0,4) AS avg_compression_time_secs,       \n       SUM(d.total_transfer_retries) AS total_transfer_retries,\n       SUM(d.distinct_error_files) AS distinct_error_files,\n       SUM(d.load_error_count) AS load_error_count\nFROM (SELECT query,\n             tbl,\n             SUM(ROWS) AS rows_inserted,\n             MAX(endtime) AS endtime,\n             datediff('microsecond',MIN(starttime),MAX(endtime)) AS insert_micro\n      FROM stl_insert\n      GROUP BY query,\n               tbl) a,\n     pg_class b,\n     pg_namespace c,\n     (SELECT b.region,\n             l.file_format,\n             b.query,\n             b.bucket as s3_bucket,\n             COUNT(DISTINCT b.bucket||b.key) AS distinct_files,\n             COUNT(DISTINCT b.slice) as used_slice_count,\n             SUM(b.transfer_size) / 1024 / 1024 AS mb_scanned,\n             SUM(b.transfer_time) AS load_micro,\n             SUM(b.compression_time) AS compression_micro,\n             datediff('microsecond',MIN('2000-01-01'::timestamp + (start_time/1000000.0)* interval '1 second'),MAX('2000-01-01'::timestamp + (end_time/1000000.0)* interval '1 second')) AS copy_duration_micro,\n             SUM(b.retries) as total_transfer_retries,\n             SUM(nvl(se.distinct_error_files,0)) AS distinct_error_files,\n             SUM(nvl(se.load_error_count,0)) AS load_error_count,\n             CASE WHEN SUM(b.transfer_size) = SUM(b.data_size) then 'N' else  'Y' end AS files_compressed\n      FROM stl_s3client b\n      INNER JOIN (select userid, query, MAX(file_format) as file_format FROM stl_load_commits GROUP BY userid, query) l ON (l.userid = b.userid and l.query = b.query)\n      LEFT OUTER JOIN (select userid, query, COUNT(DISTINCT bucket||key) AS distinct_error_files, COUNT(1) AS load_error_count FROM stl_s3client_error GROUP BY userid, query) se ON (se.userid = b.userid and se.query = b.query)\n      WHERE b.http_method = 'GET'\n      GROUP BY b.region,l.file_format,b.query,b.bucket) d,\n     stl_query q,\n     (SELECT COUNT(1) AS cluster_slice_count FROM stv_slices)\nWHERE a.tbl = b.oid\nAND   b.relnamespace = c.oid\nAND   d.query = a.query\nAND   a.query = q.query\nAND   lower(q.querytxt) LIKE '%copy %'\nGROUP BY 1,2,3,4,5,6,7,8\nORDER BY 9 DESC, 21 DESC, 1 DESC\n LIMIT 50;",
            "Signals": [
                {
                    "Signal": "files per copy are less than slice count",
                    "Criteria": "avg_files_per_copy < 64 ",
                    "PopulationCriteria": "no_of_copy > 24",
                    "Recommendation": [
                        "23"
                    ]
                },
                {
                    "Signal": "files with a small size",
                    "Criteria": "avg_file_size_mb < 10",
                    "PopulationCriteria": "no_of_copy > 24",
                    "Recommendation": [
                        "23"
                    ]
                },
                {
                    "Signal": "files are uncompressed",
                    "Criteria": "files_compressed == 'N'",
                    "PopulationCriteria": "no_of_copy > 24",
                    "Recommendation": [
                        "23"
                    ]
                }
            ],
            "Observations": []
        },
        "SpectrumPerformance": {
            "SQL": "SELECT trim(et.schemaname) AS namespace,\n       trim(et.tablename) AS external_table_name,\n       trim(lq.file_format) AS file_format,\n       trim(l.s3_bucket) AS s3_bucket,\n       CASE\n         WHEN et.compressed = 1 THEN 'Y'\n         ELSE 'N'\n       END AS is_table_compressed,\n       CASE\n         WHEN lq.is_partitioned = 't' THEN 'Y'\n         ELSE 'N'\n       END AS is_table_partitioned,\n       CASE\n         WHEN l.avg_file_splits > 1 THEN 'Y'\n         ELSE 'N'\n       END AS is_file_spittable,\n       MAX(nvl (ep.external_table_partition_count,0)) AS external_table_partition_count,\n       COUNT(1) total_query_count,\n       SUM(CASE WHEN lp.qualified_partitions < ep.external_table_partition_count THEN 1 ELSE 0 END) total_query_using_Partition_Pruning_count,\n       ROUND(SUM(CASE WHEN lp.qualified_partitions < ep.external_table_partition_count THEN 1 ELSE 0 END) / COUNT(1)::NUMERIC(38,4)*100.0,4) AS pct_of_query_using_Partition_Pruning,\n       nvl(AVG(CASE WHEN lp.qualified_partitions != 0 THEN lp.qualified_partitions END),0) AS avg_Qualified_Partitions,\n       nvl(AVG(CASE WHEN lp.qualified_partitions != 0 THEN lp.avg_assigned_partitions END),0) AS avg_Assigned_Partitions,\n       ROUND(nvl(AVG(CASE WHEN lp.qualified_partitions != 0 THEN lq.avg_request_parallelism END),0),4) AS avg_Parallelism,\n       nvl(AVG(CASE WHEN lp.qualified_partitions != 0 THEN lq.files END),0) AS avg_Files,\n       AVG(lq.splits) AS avg_Split,\n       ROUND(AVG(l.avg_max_file_size_mb),4) AS avg_max_file_size_mb,\n       ROUND(AVG(l.avg_file_size_mb),4) AS avg_file_size_mb,\n       ROUND(AVG(elapsed / 1000000::NUMERIC(38,4)),4) avg_Elapsed_sec,\n       ROUND(SUM(elapsed / 1000000::NUMERIC(38,4)),4) Total_Elapsed_sec,\n       SUM(nvl(r.spectrum_scan_error_count,0)) AS total_spectrum_scan_error_count,\n       AVG(nvl(r.spectrum_scan_error_count,0)) AS avg_spectrum_scan_error_count,\n       SUM(CASE WHEN lp.qualified_partitions = 0 THEN 1 ELSE 0 END) Queries_Using_No_S3Files\nFROM svl_s3query_summary lq\n  INNER JOIN stl_query q\n          ON (q.userid = lq.userid\n         AND q.query = lq.query\n         AND q.xid = lq.xid\n         AND q.pid = lq.pid)\n  INNER JOIN svv_external_tables et ON (q.database || '_' || et.schemaname || '_' || et.tablename = replace (replace (lq.external_table_name,'S3 Scan ',''),'S3 Subquery ',''))\n  LEFT OUTER JOIN (SELECT schemaname,\n                          tablename,\n                          COUNT(1) AS external_table_partition_count\n                   FROM svv_external_partitions\n                   GROUP BY schemaname,\n                            tablename) ep\n               ON (ep.schemaname = et.schemaname\n              AND ep.tablename = et.tablename)\n  LEFT OUTER JOIN svl_s3partition_summary lp ON lq.query = lp.query\n  LEFT OUTER JOIN (SELECT query,\n                          bucket AS s3_bucket,\n                          AVG(max_file_size / 1000000.0) AS avg_max_file_size_mb,\n                          AVG(avg_file_size / 1000000.0) AS avg_file_size_mb,\n                          AVG(generated_splits) AS avg_file_splits\n                   FROM svl_s3list\n                   GROUP BY query,\n                            bucket) l ON (lq.query = l.query)\n  LEFT OUTER JOIN (SELECT query,\n                          userid,\n                          count(1) AS spectrum_scan_error_count\n                   FROM svl_spectrum_scan_error\n                   GROUP BY query, userid) r ON (lq.query = r.query and lq.userid = r.userid)\nWHERE lq.starttime >= dateadd(day,- 7,CURRENT_DATE)\nAND   lq.aborted = 0\nGROUP BY 1,2,3,4,5,6,7\nORDER BY Total_Elapsed_sec DESC LIMIT 50;",
            "Signals": [
                {
                    "Signal": "high count of spectrum queries not using partition pruning",
                    "Criteria": "pct_of_query_using_partition_pruning < 5",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "27"
                    ]
                }
            ],
            "Observations": []
        },
        "DataShareProducerObject": {
            "SQL": "WITH shared_table AS\n(\n  SELECT t.schema_name|| '.' ||t.table_name AS object_name,\n         MAX(t.table_rows) AS table_rows,\n         MAX(v.eventtime) AS last_vacuum_date,\n         MAX(CASE WHEN v.is_recluster = 0 THEN 'N' WHEN v.is_recluster = 1 THEN 'Y' ELSE NULL END) AS is_last_vacuum_recluster,\n         MAX(i.num_insert_operation) AS num_insert_operation,\n         MAX(i.total_inserted_rows) AS total_inserted_rows,\n         MAX(i.last_insert_date) AS last_insert_date,\n         MAX(d.num_delete_operation) AS num_delete_operation,\n         MAX(d.total_deleted_rows) AS total_deleted_rows,\n         MAX(d.last_delete_date) AS last_delete_date\n  FROM (SELECT DISTINCT (stv_tbl_perm.id) table_id,\n               TRIM(pg_database.datname) AS \"database\",\n               TRIM(pg_namespace.nspname) AS schema_name,\n               TRIM(relname) AS table_name,\n               reltuples::bigint AS table_rows\n        FROM stv_tbl_perm\n          JOIN pg_database ON pg_database.oid = stv_tbl_perm.db_id\n          JOIN pg_class ON pg_class.oid = stv_tbl_perm.id\n          JOIN pg_namespace ON pg_namespace.oid = pg_class.relnamespace\n        WHERE schema_name NOT IN ('pg_internal','pg_catalog','pg_automv')) t\n    LEFT OUTER JOIN stl_vacuum v\n                 ON (t.table_id = v.table_id\n                AND v.status NOT LIKE 'Skip%')\n    LEFT OUTER JOIN (SELECT tbl,\n                            COUNT(1) AS num_insert_operation,\n                            SUM(ROWS) AS total_inserted_rows,\n                            MAX(endtime) AS last_insert_date\n                     FROM stl_insert\n                     GROUP BY 1) i ON (i.tbl = t.table_id)\n    LEFT OUTER JOIN (SELECT tbl,\n                            COUNT(1) AS num_delete_operation,\n                            SUM(ROWS) AS total_deleted_rows,\n                            MAX(endtime) AS last_delete_date\n                     FROM stl_delete\n                     GROUP BY 1) d ON (d.tbl = t.table_id)\n  GROUP BY 1\n)\nSELECT d.share_type,\n       d.share_name,\n       case when d.include_new = true then 'True' when d.include_new = false then 'False' else null end as include_new,\n       d.producer_account,\n       d.object_type,\n       replace(substring(d.object_name,1,strpos (d.object_name,'.')),'.','') AS namespace,\n       substring(d.object_name,strpos (d.object_name,'.') +1) AS object_name,\n       ti.table_rows,\n       ti.last_vacuum_date,\n       ti.is_last_vacuum_recluster,\n       ti.num_insert_operation,\n       ti.total_inserted_rows,\n       ti.last_insert_date,\n       ti.num_delete_operation,\n       ti.total_deleted_rows,\n       ti.last_delete_date,\n       CASE\n         WHEN mi.is_stale = 't' THEN 'Y'\n         WHEN mi.is_stale = 'f' THEN 'N'\n         ELSE NULL\n       END AS is_mv_stale,\n       CASE\n         WHEN mi.state = 1 THEN 'Y'\n         WHEN mi.state <> 1 THEN 'N'\n         ELSE NULL\n       END AS is_mv_incremental_refresh,\n       CASE\n         WHEN mi.autorefresh = 1 THEN 'Y'\n         WHEN mi.autorefresh <> 1 THEN 'N'\n         ELSE NULL\n       END AS is_mv_auto_refresh\nFROM svv_datashare_objects d\n  LEFT OUTER JOIN shared_table ti ON (d.object_name = ti.object_name)\n  LEFT OUTER JOIN stv_mv_info mi ON (d.object_name = mi.schema|| '.' ||mi.name)\nWHERE d.share_type = 'OUTBOUND'\nORDER BY 2,3,5,4;",
            "Signals": [
                {
                    "Signal": "large table shared by a producer",
                    "Criteria": "table_rows > 50000000",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "28",
                        "32"
                    ]
                },
                {
                    "Signal": "materialized view shared by a producer is doing a full refresh",
                    "Criteria": "is_mv_incremental_refresh == 'N'",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "34"
                    ]
                }
            ],
            "Observations": []
        },
        "DataShareConsumerUsage": {
            "SQL": "with consumer_activity as \n(\nselect uc.userid\n      ,u.usename as db_username\n      ,uc.pid\n      ,uc.xid\n      ,min(uc.recordtime) as request_start_date\n      ,max(uc.recordtime) as request_end_date\n      ,datediff('milliseconds',min(uc.recordtime),max(uc.recordtime))::NUMERIC(38,4) / 1000 as request_duration_secs\n      ,nvl(count(distinct uc.transaction_uid),0) as unique_transaction\n      ,nvl(count(uc.request_id),0) as total_usage_consumer_count\n      ,sum(case when trim(uc.error) = '' then 0 else 1 end) as request_error_count\n  from svl_datashare_usage_consumer uc\n  inner join pg_user u on (u.usesysid = uc.userid)\ngroup by 1,2,3,4\n)\n,consumer_query as (\nselect trim(q.\"database\") as dbname\n      ,trim(cu.db_username) as db_username\n      ,cu.request_start_date::date as request_date\n      ,cu.request_duration_secs\n      ,datediff('milliseconds',cu.request_end_date,q.starttime)::NUMERIC(38,4) / 1000  as request_interval_secs\n      ,datediff('milliseconds',q.starttime,q.endtime)::NUMERIC(38,4) / 1000 as query_execution_secs\n      ,datediff('milliseconds',request_start_date,q.endtime)::NUMERIC(38,4) / 1000 as total_execution_secs\n      ,q.query\n      ,cu.unique_transaction\n      ,cu.total_usage_consumer_count\n      ,cu.request_error_count      \nfrom consumer_activity cu \ninner join stl_query q on (q.xid = cu.xid and q.pid = cu.pid and q.userid = cu.userid)\n)\n,consumer_query_aggregate as (\nselect cq.request_date\n      ,cq.dbname\n      ,cq.db_username\n\t  ,avg(cq.request_duration_secs) as avg_request_duration_secs\n      ,sum(cq.request_duration_secs) as total_request_duration_secs\n\t  ,avg(cq.request_interval_secs) as avg_request_interval_secs\n      ,sum(cq.request_interval_secs) as total_request_interval_secs\n\t  ,avg(cq.query_execution_secs) as avg_query_execution_secs\n      ,sum(cq.query_execution_secs) as total_query_execution_secs\n\t  ,avg(cq.total_execution_secs) as avg_execution_secs\n      ,sum(cq.total_execution_secs) as total_execution_secs\n      ,count(cq.query) as query_count\n      ,sum(cq.unique_transaction) as total_unique_transaction\n      ,sum(cq.total_usage_consumer_count) as total_usage_consumer_count\n      ,sum(cq.request_error_count) as total_request_error_count \n  from consumer_query cq\ngroup by 1,2,3\n)\n,consumer_query_request_percentile AS (\nSELECT cq.request_date\n      ,cq.dbname\n      ,cq.db_username\n\t  ,percentile_cont(0.8) within GROUP ( ORDER BY request_duration_secs) AS p80_request_sec\n\t  ,percentile_cont(0.9) within GROUP ( ORDER BY request_duration_secs) AS p90_request_sec\n\t  ,percentile_cont(0.99) within GROUP ( ORDER BY request_duration_secs) AS p99_request_sec\n  from consumer_query cq\ngroup by 1,2,3\n)\nselect cqa.request_date\n      ,cqa.dbname\n      ,cqa.db_username\n      ,cqa.query_count\n\t  ,cqa.avg_query_execution_secs\n      ,cqa.total_query_execution_secs\n\t  ,cqa.avg_execution_secs\n      ,cqa.total_execution_secs\n\t  ,cqa.avg_request_duration_secs\n\t  ,cqrp.p80_request_sec\n\t  ,cqrp.p90_request_sec\n\t  ,cqrp.p99_request_sec\n      ,cqa.total_request_duration_secs\n\t  ,cqa.avg_request_interval_secs\n      ,cqa.total_request_interval_secs\t  \n      ,cqa.total_unique_transaction\n      ,cqa.total_usage_consumer_count\n      ,cqa.total_request_error_count\n  from consumer_query_aggregate cqa\n  inner join consumer_query_request_percentile cqrp on (cqa.request_date = cqrp.request_date and cqa.dbname = cqrp.dbname and cqa.db_username = cqrp.db_username)\norder by 1,2,3;",
            "Signals": [
                {
                    "Signal": "long running metadata sync",
                    "Criteria": "avg_request_duration_secs > 60",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "28",
                        "34",
                        "32"
                    ]
                }
            ],
            "Observations": []
        },
        "ATOWorkerActions": {
            "SQL": "WITH latest_worker_action AS\n(\n  SELECT table_id,\n         TYPE,\n         trim(status) as status,\n         eventtime,\n         ROW_NUMBER() OVER (PARTITION BY table_id,TYPE ORDER BY eventtime DESC) AS rnum\n  FROM svl_auto_worker_action w\n)\nSELECT trim(t.database_name) AS dbname,\n       t.schema_name AS namespace,\n       t.table_name,\n       w.type,\n       w.status AS latest_status,\n       w.eventtime\nFROM latest_worker_action w\n  INNER JOIN (SELECT DISTINCT(stv_tbl_perm.id) AS table_id\n                    ,TRIM(pg_database.datname) AS database_name\n                    ,TRIM(pg_namespace.nspname) AS schema_name\n                    ,TRIM(relname) AS table_name\n                FROM stv_tbl_perm\n              INNER JOIN pg_database on pg_database.oid = stv_tbl_perm.db_id\n              INNER JOIN pg_class on pg_class.oid = stv_tbl_perm.id\n              INNER JOIN pg_namespace on pg_namespace.oid = pg_class.relnamespace\n               WHERE schema_name NOT IN ('pg_internal', 'pg_catalog','pg_automv')) t\n          ON (t.table_id = w.table_id AND w.rnum = 1)\nORDER BY 1,2,3,4;",
            "Signals": [
                {
                    "Signal": "column encoding on table is not set to auto",
                    "Criteria": "type=='encode' and latest_status.str.startswith('Abort:This table is not AUTO')",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "4"
                    ]
                }
            ],
            "Observations": []
        },
        "WorkloadEvaluation": {
            "SQL": "WITH hour_list AS\n(\n  SELECT DATE_TRUNC('m',starttime) start_hour,\n         dateadd('m',1,start_hour) AS end_hour\n  FROM stl_query q\n  WHERE starttime >= (getdate() -7)\n  GROUP BY 1\n),\nscan_sum AS\n(\n  SELECT query,\n         segment,\n         SUM(bytes) AS bytes\n  FROM stl_scan\n  WHERE userid > 1\n  GROUP BY query,\n           segment\n),\nscan_list AS\n(\n  SELECT query,\n         MAX(bytes) AS max_scan_bytes\n  FROM scan_sum\n  GROUP BY query\n),\nquery_list AS\n(\n  SELECT w.query,\n         exec_start_time,\n         exec_end_time,\n         ROUND(total_exec_time / 1000 / 1000.0,3) AS exec_sec,\n         max_scan_bytes,\n         CASE\n           WHEN max_scan_bytes < 100000000 THEN 'small'\n           WHEN max_scan_bytes BETWEEN 100000000 AND 500000000000 THEN 'medium'\n           WHEN max_scan_bytes > 500000000000 THEN 'large'\n         END AS size_type\n  FROM stl_wlm_query w,\n       scan_list sc\n  WHERE sc.query = w.query\n),\nworkload_exec_seconds  AS\n(\nselect \ncount(*) as query_cnt, \nSUM(CASE WHEN size_type = 'small' THEN  exec_sec  ELSE 0 END) AS small_workload_exec_sec_sum,\nSUM(CASE WHEN size_type = 'medium' THEN  exec_sec  ELSE 0 END) AS medium_workload_exec_sec_sum,\nSUM(CASE WHEN size_type = 'large' THEN  exec_sec  ELSE 0 END) AS large_workload_exec_sec_sum,\n  \nAVG(CASE WHEN size_type = 'small' THEN  exec_sec  ELSE 0 END) AS small_workload_exec_sec_avg,\nAVG(CASE WHEN size_type = 'medium' THEN  exec_sec  ELSE 0 END) AS medium_workload_exec_sec_avg,\nAVG(CASE WHEN size_type = 'large' THEN  exec_sec  ELSE 0 END) AS large_workload_exec_sec_avg,\n  \nMAX(CASE WHEN size_type = 'small' THEN  exec_sec  ELSE 0 END) AS small_workload_exec_sec_max,\nMAX(CASE WHEN size_type = 'medium' THEN  exec_sec  ELSE 0 END) AS medium_workload_exec_sec_max,\nMAX(CASE WHEN size_type = 'large' THEN  exec_sec  ELSE 0 END) AS large_workload_exec_sec_max,\n  \n  MIN(CASE WHEN size_type = 'small' THEN  exec_sec  ELSE 0 END) AS small_workload_exec_sec_min,\nMIN(CASE WHEN size_type = 'medium' THEN  exec_sec  ELSE 0 END) AS medium_workload_exec_sec_min,\nMIN(CASE WHEN size_type = 'large' THEN  exec_sec  ELSE 0 END) AS large_workload_exec_sec_min,\n  \nAVG(CASE WHEN size_type = 'small' THEN  max_scan_bytes  ELSE 0 END) AS small_workload_max_scan_bytes_avg,\nAVG(CASE WHEN size_type = 'medium' THEN  max_scan_bytes  ELSE 0 END) AS medium_workload_max_scan_bytes_avg,\nAVG(CASE WHEN size_type = 'large' THEN  max_scan_bytes  ELSE 0 END) AS large_workload_max_scan_bytes_avg,\n\n  (small_workload_exec_sec_sum+medium_workload_exec_sec_sum+large_workload_exec_sec_sum) as total_workload_exec_sec_sum,\nsmall_workload_exec_sec_sum/(total_workload_exec_sec_sum*1.00) as Small_workload_perc,\nmedium_workload_exec_sec_sum/(total_workload_exec_sec_sum*1.00) as Medium_workload_perc,\nlarge_workload_exec_sec_sum/(total_workload_exec_sec_sum*1.00) as Large_workload_perc\nfrom query_list\n)\n,query_list_2 AS\n(\n  SELECT start_hour,\n         query,\n         size_type,\n         max_scan_bytes,\n         exec_sec,\n         exec_start_time,\n         exec_end_time\n  FROM hour_list h,\n       query_list q\n  WHERE exec_start_time BETWEEN start_hour AND end_hour\n  OR    exec_end_time BETWEEN start_hour AND end_hour\n  OR    (exec_start_time < start_hour AND exec_end_time > end_hour)\n)\n\n,hour_list_agg AS\n(\n  SELECT start_hour,\n         SUM(CASE WHEN size_type = 'small' THEN 1 ELSE 0 END) AS small_query_cnt,\n         SUM(CASE WHEN size_type = 'medium' THEN 1 ELSE 0 END) AS medium_query_cnt,\n         SUM(CASE WHEN size_type = 'large' THEN 1 ELSE 0 END) AS large_query_cnt,\n         COUNT(*) AS tot_query_cnt\n  FROM query_list_2\n  GROUP BY start_hour\n) \n,utilization_perc AS\n(\nSELECT trunc(start_hour) AS sample_date,\n       ROUND(100*SUM(CASE WHEN tot_query_cnt > 0 THEN 1 ELSE 0 END) / 1440.0,1) AS all_query_activite_perc,\n       ROUND(100*SUM(CASE WHEN small_query_cnt > 0 THEN 1 ELSE 0 END) / 1440.0,1) AS small_query_activite_perc,\n       ROUND(100*SUM(CASE WHEN medium_query_cnt > 0 THEN 1 ELSE 0 END) / 1440.0,1) AS medium_query_activite_perc,\n       ROUND(100*SUM(CASE WHEN large_query_cnt > 0 THEN 1 ELSE 0 END) / 1440.0,1) AS large_query_activite_perc,\n       MIN(start_hour) AS start_hour,\n       MAX(start_hour) AS end_hour\nFROM hour_list_agg\nGROUP BY 1\n)\n\n,activity_perc as \n(\nSelect avg(small_query_activite_perc) AS AVG_small_query_activity_perc, \navg(medium_query_activite_perc) AS AVG_medium_query_activity_perc,\navg(large_query_activite_perc) AS AVG_large_query_activity_perc\nFROM utilization_perc\n)\n\n,mincount AS\n(\nSELECT trunc(start_hour) AS sample_date,\n       SUM(CASE WHEN tot_query_cnt > 0 THEN 1 ELSE 0 END) AS tot_query_minute,\n       SUM(CASE WHEN small_query_cnt > 0 THEN 1 ELSE 0 END) AS small_query_minute,\n       SUM(CASE WHEN medium_query_cnt > 0 THEN 1 ELSE 0 END) AS medium_query_minute,\n       SUM(CASE WHEN large_query_cnt > 0 THEN 1 ELSE 0 END) AS large_query_minute,\n       MIN(start_hour) AS start_hour,\n       MAX(start_hour) AS end_hour\nFROM hour_list_agg\nGROUP BY 1\n),avgmincount AS\n(\nSelect avg(small_query_minute) avg_small_query_minute, avg(medium_query_minute) avg_medium_query_minute, avg(large_query_minute) avg_large_query_minute\n  from mincount\n)\n,final_output AS \n(\nSelect   small_workload_perc , medium_workload_perc,    large_workload_perc,    \n  avg_small_query_activity_perc,    avg_medium_query_activity_perc,    avg_large_query_activity_perc,    \n  avg_small_query_minute,    avg_medium_query_minute,    avg_large_query_minute,\n  \n  small_workload_exec_sec_avg, medium_workload_exec_sec_avg, large_workload_exec_sec_avg,\n  small_workload_exec_sec_max, medium_workload_exec_sec_max, large_workload_exec_sec_max,\n  small_workload_exec_sec_min, medium_workload_exec_sec_min, large_workload_exec_sec_min,\n   total_query_cnt, total_small_query_cnt, \n  total_medium_query_cnt,total_large_query_cnt, \n  small_workload_max_scan_bytes_avg, \n  medium_workload_max_scan_bytes_avg, \n  large_workload_max_scan_bytes_avg\nfrom activity_perc a, avgmincount b, workload_exec_seconds c, (  select count(*) as total_query_cnt, sum(case when size_type = 'small' then 1 else 0 end) as  total_small_query_cnt, \n                                                              sum(case when size_type = 'medium' then 1 else 0 end) as  total_medium_query_cnt, \n                                                              sum(case when size_type = 'large' then 1 else 0 end) as  total_large_query_cnt\n                                                              from query_list )  d\nwhere 1=1\n\n)\nselect workloadtype, (perc_of_total_workload*100.00) as perc_of_total_workload, perc_duration_in_day, Total_query_minutes_in_day \n,workload_exec_sec_avg, workload_exec_sec_min, workload_exec_sec_max,query_cnt,scan_bytes_avg\nfrom\n(\nselect \n'Small' as workloadtype,\nsmall_workload_perc as perc_of_total_workload,\navg_small_query_activity_perc as perc_duration_in_day,\navg_small_query_minute as Total_query_minutes_in_day, \n small_workload_exec_sec_avg as  workload_exec_sec_avg,\n  small_workload_exec_sec_max as  workload_exec_sec_max, \n   small_workload_exec_sec_min as  workload_exec_sec_min,\n total_small_query_cnt as query_cnt, \n  small_workload_max_scan_bytes_avg as scan_bytes_avg,\n 1 as id\nfrom final_output\nunion\nselect \n'Meduim' as workloadtype,\nmedium_workload_perc as perc_of_total_workload,\navg_medium_query_activity_perc as perc_duration_in_day,\navg_medium_query_minute as Total_query_minutes_in_day ,\n  medium_workload_exec_sec_avg as  workload_exec_sec_avg,\n    medium_workload_exec_sec_max as  workload_exec_sec_max, \n   medium_workload_exec_sec_min as  workload_exec_sec_min,\n total_medium_query_cnt as query_cnt, \n  medium_workload_max_scan_bytes_avg as scan_bytes_avg,\n  2 as id\nfrom final_output\nunion\nselect \n'Large' as workloadtype,\nlarge_workload_perc as perc_of_total_workload,\navg_large_query_activity_perc as perc_duration_in_day,\navg_large_query_minute as Total_query_minutes_in_day, \n  large_workload_exec_sec_avg as  workload_exec_sec_avg,\n   large_workload_exec_sec_max as  workload_exec_sec_max, \n   large_workload_exec_sec_min as  workload_exec_sec_min,\n total_large_query_cnt as query_cnt, \n  large_workload_max_scan_bytes_avg as scan_bytes_avg,\n  3 as id\n from final_output\n  ) a order by id asc;",
            "Signals": [
                {
                    "Signal": "Evaluate Workload for Serverless. Cluster is busy less than 75% of the time in a day.",
                    "Criteria": "select sum(total_query_minutes_in_day)/1440 from df  || *100 < 75",
                    "PopulationCriteria": "",
                    "Recommendation": [
                        "35"
                    ]
                }
            ],
            "Observations": []
        }
    },
    "Recommendations": {
        "1": {
            "text": "For additional scalability, migrate to the RA3 node type or serverless",
            "description": "Documentation: <br><ul><a href='https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#rs-upgrading-to-ra3' target='_blank'>Upgrading to RA3 node type</a><a href='https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-whatis.html' target='_blank'>What is Amazon Redshift Serverless?</a><a href='https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-migration.html' target='_blank'> Migrating a provisioned cluster to Amazon Redshift Serverless</a></ul>Best Practices:<ul><a href='https://aws.amazon.com/premiumsupport/knowledge-center/redshift-ra3-node-type/' target='_blank'>How do I migrate my Amazon Redshift cluster to an RA3 node type?</a><a href='https://aws.amazon.com/blogs/big-data/use-amazon-redshift-ra3-with-managed-storage-in-your-modern-data-architecture/' target='_blank'>Use Amazon Redshift RA3 with managed storage in your modern data architecture</a></ul>",
            "effort": "Small"
        },
        "2": {
            "text": "For busy clusters, schedule a VACUUM DELETE ",
            "description": "Amazon Redshift automatically performs a DELETE ONLY vacuum in the background, so you rarely, if ever, need to run a DELETE ONLY vacuum.  However, this operation only runs when the cluster is idle.  If your cluster is busy, you should schedule this operation to minimize I/O operations. <br><br> A VACUUM DELETE reclaims disk space occupied by rows that were marked for deletion by previous UPDATE and DELETE operations, and compacts the table to free up the consumed space. A DELETE ONLY vacuum operation doesn't sort table data.<br><br>Documentation: <br><ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/t_Reclaiming_storage_space202.html#automatic-table-delete' target='_blank'>Automatic vacuum delete</a></ul>",
            "effort": "Small"
        },
        "3": {
            "text": "For busy clusters, schedule ANALYZE commands",
            "description": "Stale or missing table statistics reduce sort key effectiveness and may cause the optimizer not to use the optimal query path for the affected queries. A higher value of the stats_off column in SVV_TABLE_INFO means that statistics is more out of date hence it is recommended to check tables with >= 10. While Redshift automatically analyzes your data, this occurs when the cluster is not busy.  If you have a busy cluster or if there is significant write operations/changes on the table (ie DDL change) and access to the data is immediately required, explicitly execute or schedule ANALYZE commands. Consider <code>ANALYZE PREDICATE COLUMNS</code> or <a href='https://github.com/awslabs/amazon-redshift-utils/blob/master/src/StoredProcedures/sp_analyze_minimal.sql' target ='_blank'>sp_analyze_minimal</a> if you have a wide table (100s of columns) for a faster execution.<br><br>Documentation: <br><ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/t_Analyzing_tables.html' target='_blank'>Analyzing tables</a></ul>",
            "effort": "Medium"
        },
        "4": {
            "text": "If you are running out of storage or to reduce I/O overhead, review compression encodings",
            "description": "Compression is a column-level operation that reduces the size of data when it is stored. Compression conserves storage space and reduces the size of data that is read from storage, which reduces the amount of disk I/O and therefore improves query performance. As a best practice, leverage AZ64 for numeric and data data types and ZSTD for character data types where possible or use ENCODE AUTO to automatically manage the compression encoding for all columns in the table. <br><br>For monitoring actions of automatic table optimization, a system view <a href='https://docs.aws.amazon.com/redshift/latest/dg/r_SVV_ALTER_TABLE_RECOMMENDATIONS.html' target='_blank'>SVV_ALTER_TABLE_RECOMMENDATIONS</a> records the current Amazon Redshift Advisor recommendations for tables while <a href='https://docs.aws.amazon.com/redshift/latest/dg/r_SVL_AUTO_WORKER_ACTION.html' target='_blank'>SVL_AUTO_WORKER_ACTION</a> shows an audit log of all the actions taken by the Amazon Redshift, and the previous state of the table.<br><br>To change the encoding of the table to AUTO, the following command can be executed:<br><code>ALTER TABLE [table_name] ALTER ENCODE AUTO;<br></code><br>Documentation: <br><ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/t_Compressing_data_on_disk.html target='_blank'>Working with column compression</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/t_Verifying_data_compression.html' target='_blank'>Testing compression encodings</a></ul>",
            "effort": "Medium"
        },
        "5": {
            "text": "If you are running out of storage or memory, add more compute nodes using resize",
            "description": "When using node types like DC2, you may run into situations where you are out of storage.  In some cases, both DC2 and RA3, where you see heavy disk spill, resizing your cluster can make more memory available and speed up query execution. <br><br> Documentation: <a href='https://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html#elastic-resize' target='_blank'>Elastic Resize operation</a> <br> Blog:<a href='https://aws.amazon.com/blogs/big-data/accelerate-resize-and-encryption-of-amazon-redshift-clusters-with-faster-classic-resize/' target = '_blank'>Accelerate resize and encryption of Amazon Redshift clusters with Faster Classic Resize</a>",
            "effort": "Small"
        },
        "6": {
            "text": "Unload infrequently accessed data to S3 and query using Redshift Spectrum.",
            "description": "Redshift Spectrum allows you to query exabytes of data in your Amazon S3 data lake, without loading or moving objects. Infrequently accessed data (also known as COLD data) can reside/be offloaded in Amazon S3 and be queried in Amazon Redshift using Redshift Spectrum.<br>You can also define a spectrum usage limit either as a daily, weekly, and monthly usage limits and define actions that Amazon Redshift automatically takes if the limits defined by you are reached. The action taken could be logging the event to a system table, creating an alert for notification or disabling the feature.<br><br>Blogs:<ul><a href='https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/' target='_blank'>Best Practices for Amazon Redshift Spectrum</a><a href='https://aws.amazon.com/blogs/big-data/manage-and-control-your-cost-with-amazon-redshift-concurrency-scaling-and-spectrum/' target='_blank'>Manage and control your cost with Amazon Redshift Concurrency Scaling and Spectrum</a><a href='https://aws.amazon.com/blogs/big-data/working-with-nested-data-types-using-amazon-redshift-spectrum/' target='_blank'>Working with nested data types using Amazon Redshift Spectrum</a></ul>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/c-getting-started-using-spectrum.html' target ='_blank'>Getting started with Amazon Redshift Spectrum</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/tutorial-query-nested-data.html' target ='_blank'>Tutorial: Querying nested data with Amazon Redshift Spectrum</a></ul>",
            "effort": "Medium"
        },
        "7": {
            "text": "To improve query performance, choose a better sort key",
            "description": "For large tables, adding a sort key can speed up queries with predicates. Review commonly filtered fields and add a sort key:<br><code>ALTER TABLE [table_name] ALTER SORTKEY ([column1], [columnN])</code><br><br>For small tables (&lt;5 million rows or less), sort keys are not effective and will add maintenance and storage overhead:<br><code>ALTER TABLE [table_name] ALTER SORTKEY NONE;</code><br><br>For unknown table sizes or access pattern, you can set the sort key to AUTO which allows Redshift to define the optimal sort key:<br><code>ALTER TABLE [table_name] ALTER SORTKEY AUTO;</code>",
            "effort": "Small"
        },
        "8": {
            "text": "To improve query performance, choose the best distribution style",
            "description": "Heavy skew (where the ratio of the number of rows on a slice is more than 4x of another slice) can cause queries to perform slower as they are waiting on the compute node with the most data to complete before returning the result set. If the first node is skewed, this may be due to using a distribution key containing nulls. To change the column used as a distribution key, the following command can be executed:<br><code>ALTER TABLE [table_name] ALTER DISTSTYLE KEY DISTKEY <high_cardinality_column>;</code><br><br>For small tables (< 5mill rows) e.g. reference or dimension tables that are frequently used in JOINS consider an ALL distribution style which puts a complete copy of the table on each node. <br><code>ALTER TABLE [table_name] ALTER DISTSTYLE AUTO;</code><br><br>To manage the distribution style automatically, it's recommended that you create your tables with DISTSTYLE AUTO. Amazon Redshift initially assigns ALL distribution to a small table, then changes to EVEN distribution when the table grows larger. It automatically monitors the workload on the cluster and over time may change the distribution of your data to have a KEY-based distribution style. To change the distribution style of a table, the following command can be executed:<br><code>ALTER TABLE [table_name] ALTER DISTSTYLE AUTO;</code><br><br>Use of temporal columns such as sold_date, event_date etc whenever there is a minimal or peak activity will likely cause a huge variance compared to normal activity that generates that data thus causing data skewness. A low skew value indicates that table data is properly distributed. If a table has a skew value of 4.00 or higher, consider modifying its data distribution style.<br><br>Blog:<ul> <a href='https://aws.amazon.com/blogs/big-data/amazon-redshift-engineerings-advanced-table-design-playbook-distribution-styles-and-distribution-keys/' target='_blank'>Amazon Redshift Engineering Advanced Table Design Playbook: Distribution Styles and Distribution Keys</a><a href='https://aws.amazon.com/blogs/big-data/automate-your-amazon-redshift-performance-tuning-with-automatic-table-optimization/' target='_blank'>Automate your Amazon Redshift performance tuning with automatic table optimization</a></ul>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html' target='_blank'>Choose the best distribution style</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/t_Creating_tables.html#ato-enabling' target='_blank'>Enabling automatic table optimization</a></ul>",
            "effort": "Medium"
        },
        "9": {
            "text": "To improve query performance, remove nested loop joins (cross-joins)",
            "description": "Review queries that makes use of cross-joins and remove them if possible by adding a join condition.Documentation: <ul><a href=https://docs.aws.amazon.com/redshift/latest/dg/query-performance-improvement-opportunities.html#nested-loop' target='_blank'>Query Performance Improvement Opportunities</a></ul>",
            "effort": "Medium"
        },
        "10": {
            "text": "To improve query performance, remove the encoding on the first column of a sort key",
            "description": "It is a best practice to not encode the first column of a sort key for optimal query performance. Using ENCODE AUTO to automatically manage the compression encoding for all columns in the table also ensures this. To change the encoding of the table to AUTO, the following command can be executed:<br><code>ALTER TABLE [table_name] ALTER ENCODE AUTO;</code><br><br>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/t_Verifying_data_compression.html' target='_blank'>Testing compression encodings</a></ul>",
            "effort": "Small"
        },
        "11": {
            "text": "To optimize memory consumption and avoid disk spill, reduce varchar fields to be inline with their max length",
            "description": "Use caution when setting a large length on character type fields such as VARCHAR. When allocating memory for queries using these fields, Amazon Redshift will allocate based on the maximum values and queries are more likely to spill to disk.<br>To change the length of the column, the following command can be executed:<br><code>ALTER TABLE [table_name] ALTER COLUMN [column_name] TYPE varchar([size])</code><br><br>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-smallest-column-size.html' target='_blank'>Use the smallest possible column size</a></ul>",
            "effort": "Small"
        },
        "12": {
            "text": "To save cost, remove extra compute nodes using elastic or classic resize",
            "description": "You many have more compute than you need.  If you can reduce the size of your cluster and maintain your SLAs, you may be able to save money.<br><br>In addition, enable the concurrency scaling feature and leverage the 1 hour free credit / day on a given WLM queue. Apply the necessary concurrency scaling usage limits.<br>Use of concurrency scaling allows the cluster to handle spiky compute needs and allow for queries in WLM queues in which its enabled to get consistent performance.<br><br>Blogs:<ul><a href='https://aws.amazon.com/blogs/big-data/accelerate-resize-and-encryption-of-amazon-redshift-clusters-with-faster-classic-resize/' target='_blank'> Accelerate resize and encryption of Amazon Redshift clusters with Faster Classic Resize</a></ul>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html#elastic-resize' target='_blank'>Elastic Resize operation</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html' target='_blank'>Configuring concurrency scaling queues</a><a href='https://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-usage-limits.html' target='_blank'>Managing usage limits in Amazon Redshift</a></ul>",
            "effort": "Small"
        },
        "13": {
            "text": "To improve query performance, ensure large tables with a sort key are vacuumed",
            "description": "Tables which contain a sort key but have not been vacuumed will not benefit from the sort key. While Auto Vacuum should optimize most table, in busy clusters, customers should schedule the VACUUM SORT RECLUSTER operation either as part of the regular data ingestion / transformation workload (specially those that affects a large number of rows) or as part of a recurring maintenance activity (ie weekly / monthly) based on the vacuum_sort_benefit column in SVV_TABLE_INFO.<br><br>Use the DEEP COPY approach if the table does not need to be accessible for writes. A deep copy is much faster than a vacuum. The trade off is that you should not make concurrent updates during a deep copy operation unless you can track it and move the delta updates into the new table after the process has completed.<br><br>Documentation:<ul><a href='https: //docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html' target='_blank'>VACUUM SORT</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/performing-a-deep-copy.html' target='_blank'>DEEP COPY</a></ul>",
            "effort": "Medium"
        },
        "14": {
            "text": "To improve query performance and reduce maintenance overhead, replace Interleaved Sort keys with Materialized Views",
            "description": "While <a href='https: //docs.aws.amazon.com/redshift/latest/dg/r_vacuum-decide-whether-to-reindex.html' target='_blank'>interleaved sort keys may provide some benefit</a>, for tables with multiple access patterns, Materialized Views (MVs) are a newer strategy to optimize for performance in similar situations.  MVs can be used to support not only different sorting strategies, but also calculated fields, different distribution strategies, and pre-aggregated results.  Also, tables with an interleaved sort key are not eligible for concurrency scaling and data sharing. If your use case still requires interleaved sort keys, keep in mind a <a href='https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html#vacuum-reindex' target ='_blank'>VACUUM REINDEX</a> is required to take advantage of it, but it's an expensive operations and in many cases the performance benefit is not worth the additional maintenance effort.",
            "effort": "Medium"
        },
        "15": {
            "text": "To maximize system throughput and use resources most effectively, set up automatic WLM",
            "description": "With automatic workload management (WLM), Amazon Redshift manages query concurrency and memory allocation.<br>Automatic WLM determines the amount of resources that queries need, and adjusts the concurrency based on the workload. When queries requiring large amounts of resources are in the system (for example, hash joins between large tables), the concurrency is lower. When lighter queries (such as inserts, deletes, scans, or simple aggregations) are submitted, concurrency is higher.<br><br>Blogs:<ul><a href='https://aws.amazon.com/blogs/big-data/benchmarking-the-performance-of-the-new-auto-wlm-with-adaptive-concurrency-in-amazon-redshift/' target='_blank'>Benchmark the performance of the new Auto WLM with adaptive concurrency in Amazon Redshift</a></ul>Documentation:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/automatic-wlm.html' target='_blank'>Implementing automatic WLM</a></ul>",
            "effort": "Medium"
        },
        "16": {
            "text": "Re-allocate your memory distribution to add up to 100%",
            "description": "When using manual WLM total memory allocation should equal 100% when using manual WLM. Allocating less than 100% may result in unpredictable query performance and/or inefficient use of cluster resources..<br><br>Documentation:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/cm-c-defining-query-queues.html' target='_blank'>Implementing manual WLM</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/cm-c-defining-query-queues.html#wlm-memory-percent' target='_blank'>WLM memory percent to use</a></ul>",
            "effort": "Small"
        },
        "17": {
            "text": "To improve query performance, configure no more than 20 WLM slots",
            "description": "When using manual WLM, high slot counts can result in too little memory being allocated to each query and results spilling to disk which may affect the query performance and having a lower query throughput.<br>When you use lower concurrency, query throughput is increased and overall system performance is improved for most workloads.<br><br>Documentation:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/cm-c-defining-query-queues.html' target='_blank'>Implementing manual WLM</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/cm-c-defining-query-queues.html#cm-c-defining-query-queues-concurrency-level' target='_blank'>Concurrency level</a></ul>",
            "effort": "Small"
        },
        "18": {
            "text": "To have better visibility on resource usage by workload and optimize resource allocation, create separate query queues for each workload",
            "description": "When you define multiple query queues, you can route queries to the appropriate queues at runtime<br>For example query queues can be defined for each type of workload as per below:<br>- BI / Dashboard workload<br>- ETL / Data Ingestion workload<br>- Data Science workload<br>- Adhoc workload<br>- Application workload<br><br>Documentation:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-queue-assignment-rules.html' target='_blank'>WLM queue assignment rules</a></ul>",
            "effort": "Medium"
        },
        "19": {
            "text": "For additional scalability, enable the concurrency scaling",
            "description": "Use of concurrency scaling allows the cluster to handle spiky compute needs and allow for queries in WLM queues in which its enabled to get consistent performance. Concurrency scaling is available 1 hour free per day. Additionally usage limits can be applied to manage costs.<br><br>Blogs:<ul><a href='https://aws.amazon.com/blogs/big-data/manage-and-control-your-cost-with-amazon-redshift-concurrency-scaling-and-spectrum/' target='_blank'>Manage and control your cost with Amazon Redshift Concurrency Scaling and Spectrum</a></ul>Documentation:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html' target='_blank'>Working with concurrency scaling</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling-queues.html' target='_blank'>Configuring concurrency scaling queues</a><a href='https://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-usage-limits.html' target='_blank'>Managing usage limits in Amazon Redshift</a></ul>Videos:<ul><a href='https://www.youtube.com/watch?v=-2yQsI9xJKQ' target='_blank'>Amazon Redshift Concurrency Scaling</a></ul>",
            "effort": "Small"
        },
        "20": {
            "text": "For additional scalability, enable short query acceleration",
            "description": "If you enable SQA, you can reduce or eliminate workload management (WLM) queues that are dedicated to running short queries.<br>Amazon Redshift uses a machine learning algorithm to analyze each eligible query and predict the query's execution time. By default, WLM dynamically assigns a value for the SQA maximum runtime based on analysis of your cluster's workload. Alternatively, you can specify a fixed value of 1 to 20 seconds.<br>Documentation:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/wlm-short-query-acceleration.html' target='_blank'>Working with short query acceleration</a></ul>",
            "effort": "Small"
        },
        "21": {
            "text": "To ensure queries get resources based on importance, set priorities for each WLM queue",
            "description": "Not all queries are of equal importance, and often performance of one workload or set of users might be more important.<br>When auto WLM is enabled, you can define the relative importance of queries in a workload by setting a priority value. The priority is specified for a queue and inherited by all queries associated with the queue. Amazon Redshift uses the priority when letting queries into the system, and to determine the amount of resources allocated to a query.<br>Documentation:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/query-priority.html' target='_blank'>Query priority</a></ul>",
            "effort": "Small"
        },
        "22": {
            "text": "To set query performance boundaries on workloads, add QMR rules ",
            "description": "Query monitoring rules (QMR) define metrics-based performance boundaries for WLM queues and specify what action to take when a query goes beyond those boundaries. For example, to track poorly designed queries, you might have a rule that logs queries that contain nested loops or executes for more than 2 hours. You can define up to 25 rules for each queue, with a limit of 25 rules for all queues.. You can start with Query monitoring rules templates.<br>WLM evaluates metrics every 10 seconds. If more than one rule is triggered during the same period, WLM initiates the most severe action; abort, then hop, then log.<br>The most common metrics where QMR can protect your system include:<br>* query_execution_time - identifies sub-optimal table or query design<br>* query_temp_blocks_to_disk - identifies queries using more than available memory which write intermediate results to disk (spilled memory).<br>* spectrum_scan_size_mb / spectrum_scan_row_count - a large scan may mean there is no partition pruning applied and the operation will be both costly and time consuming<br>* nested_loop_join_row_count - joins without a join condition that result in the Cartesian product of two tables are the slowest of the possible join types, use the most memory and may lead to disk spill<br>Documentation:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html' target='_blank'>WLM query monitoring rules</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/cm-c-wlm-query-monitoring-rules.html#cm-c-wlm-query-monitoring-templates' target='_blank'>Query monitoring rules templates</a></ul>",
            "effort": "Medium"
        },
        "23": {
            "text": "Increase load performance by optimizing COPY operations",
            "description": "Amazon Redshift can automatically load in parallel from multiple compressed data files. This divides the workload among the nodes in your cluster.<br>However, if you use multiple concurrent COPY commands to load one table from multiple files, Amazon Redshift performs a serialized load which is slower than loading them using one operation. Optimal COPY performance can be achieved through a file count that is a multiple of the number of node slices. For optimum parallelism, the ideal file size is 1 to 125 MB after compression.<br>Blogs:<ul><a href='https://aws.amazon.com/blogs/big-data/part-1-introducing-new-features-for-amazon-redshift-copy/' target='_blank'>Introducing new features for Amazon Redshift COPY: Part 1</a></ul>Documentation:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/t_Loading-data-from-S3.html' target='_blank'>Loading data from Amazon S3</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html' target='_blank'>Amazon Redshift best practices for loading data</a></ul>",
            "effort": "Medium"
        },
        "24": {
            "text": "To avoid disk spill, review queries and add predicates to filter tables that participate in joins, even if the predicates apply the same filters",
            "description": "Even though a filter may be redundant, the query planner can skip scanning large numbers of disk blocks when performing a join operation.<br>Below is an example where a predicate for <code>sales.saletime</code> is added to the <code>WHERE</code> clause.  The query returns the same result set without the filter because a product will always be sold after it's listed, but Amazon Redshift is able to filter the table before the join step.<br><code>select listing.sellerid, sum(sales.qtysold)</code><br><code>from sales, listing</code><br><code>where sales.salesid = listing.listid</code><br><code>and listing.listtime > '2008-12-01'</code><br><code>and sales.saletime > '2008-12-01'</code><br><code>group by 1 order by 1;</code><br><br>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/c_designing-queries-best-practices.html' target ='_blank'>Amazon Redshift best practices for designing queries</a></ul>",
            "effort": "Medium"
        },
        "25": {
            "text": "To reduce compile overhead, avoid drop/create operations in favor of delete/copy/insert",
            "description": "High compile count can be caused by queries executed against new tables.<br><br>Blogs: <ul><a href='https://aws.amazon.com/blogs/big-data/fast-and-predictable-performance-with-serverless-compilation-using-amazon-redshift/' target ='_blank'>Fast and predictable performance with serverless compilation using Amazon Redshift</a></ul>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/c-query-performance.html' target ='_blank'>Factors affecting query performance</a></ul>",
            "effort": "Medium"
        },
        "26": {
            "text": "For additional scalability, isolate your workloads using data sharing",
            "description": "Amazon Redshift data sharing provides workload isolation by allowing multiple consumers to share data seamlessly without the need to unload and load data. Implementing workload isolation allows for agility, allows sizing of clusters independently, and creates a simple cost charge-back model.<br><br>Blogs:<ul><a href='https://aws.amazon.com/blogs/big-data/sharing-amazon-redshift-data-securely-across-amazon-redshift-clusters-for-workload-isolation/' target ='_blank'>Sharing Redshift data securely for workload isolation</a><a href='https://aws.amazon.com/blogs/big-data/implementing-multi-tenant-patterns-in-amazon-redshift-using-data-sharing/' target ='_blank'>Implementing multi-tenant patterns in Amazon Redshift using data sharing</a><a href='https://aws.amazon.com/blogs/aws/cross-account-data-sharing-for-amazon-redshift/' target ='_blank'>Cross-Account Data Sharing for Amazon Redshift</a><a href='https://aws.amazon.com/blogs/big-data/security-considerations-for-amazon-redshift-cross-account-data-sharing/' target ='_blank'>Security considerations for Amazon Redshift cross-account data sharing</a><a href='https://aws.amazon.com/blogs/big-data/share-data-securely-across-regions-using-amazon-redshift-data-sharing/' target ='_blank'>Share data securely across Regions using Amazon Redshift data sharing</a></ul>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/data_sharing_intro.html' target ='_blank'>Overview of data sharing in Amazon Redshift</a><a href='https://aws.amazon.com/blogs/big-data/amazon-redshift-data-sharing-best-practices-and-considerations/' target ='_blank'>Amazon Redshift data sharing best practices and considerations</a></ul>",
            "effort": "Medium"
        },
        "27": {
            "text": "Increase read performance from Spectrum tables, by optimizing your partitioning strategy",
            "description": "When you partition your data, you can restrict the amount of data that Redshift Spectrum scans by filtering on the partition key. Amazon Redshift Spectrum can take advantage of partition pruning and skip scanning unneeded partitions and files by defining the S3 prefix based on the columns (ie transaction_date) frequently used in SQL predicate.<br>A common practice is to partition the data based on time. For example, you might choose to partition by year, month, date, and hour. If you have data coming from multiple sources, you might partition by a data source identifier and date.<br>Blog: <ul><a href=https://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-redshift-spectrum/' target ='_blank'>Best Practices for Amazon Redshift Spectrum</a></ul>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-performance.html' target ='_blank'>Improving Amazon Redshift Spectrum query performance</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html#c-spectrum-external-tables-partitioning' target ='_blank'>Partitioning Redshift Spectrum external tables</a></ul>",
            "effort": "Medium"
        },
        "28": {
            "text": "To improve query performance, redesign the large table to a time series table",
            "description": "If your data has a fixed retention period, you can organize your data as a sequence of time-series tables. In such a sequence, each table is identical but contains data for different time ranges.<br><br>You can easily remove old data simply by running a DROP TABLE command on the corresponding tables. This approach is much faster than running a large-scale DELETE process and saves you from having to run a subsequent VACUUM process to reclaim space. To hide the fact that the data is stored in different tables, you can create a UNION ALL view. When you delete old data, simply refine your UNION ALL view to remove the dropped tables. Similarly, as you load new time periods into new tables, add the new tables to the view. To signal the optimizer to skip the scan on tables that don't match the query filter, your view definition filters for the date range that corresponds to each table.<br><br>If your data requires processing of different time ranges, you can follow a similar approach but instead of a time range make use of another criteria for data segregation based on data access pattern such as geography (ie region) for example.<br><br>A sample conversion process will be:<ol>Create multiple tables based on how the data needs to be accessed (ie by week, by month, by quarter, by geography, by account, etc.)Create a view that will consolidate all the time series tables (may include the external table queried using Redshift Spectrum)Replace the query on the table with the query on the view</ol>If a query filters on the sort key, the query planner can efficiently skip all the tables that aren't used. <br><br>Documentation: <br><ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/vacuum-time-series-tables.html' target='_blank'>Using time series tables</a></ul>",
            "effort": "Medium"
        },
        "29": {
            "text": "Increase load performance by avoiding or minimizing multiple single row insert statements",
            "description": "A data ingestion / loading process that runs a single row insert or multi row insert statement uses only the leader node for each execution and is slow when used to load large amounts of data compared to other data ingestion / loading options like a COPY command which engages compute nodes in parallel. Data compression is also inefficient when you add data only one row or a few rows at a time.<br><br>Redesign the data ingestion process to replace multiple single row insert statements to either use one of the following:<ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-use-copy.html' target='_blank'>Option 1: COPY command</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-column-mapping.html#copy-column-list' target='_blank'>Option 2: COPY command with column mapping</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/r_ALTER_TABLE_APPEND.html' target='_blank'>Option 3: ALTER TABLE APPEND</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-bulk-inserts.html' target='_blank'>Option 4: Bulk Insert</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-multi-row-inserts.html' target='_blank'>Option 5: Multi Row Insert</a></ul>Documentation: <br><ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html' target='_blank'>Amazon Redshift best practices for loading data</a><a href='https://docs.aws.amazon.com/redshift/latest/dg/t_Updating_tables_with_DML_commands.html' target='_blank'>Updating tables with DML commands</a></ul>",
            "effort": "Medium"
        },
        "30": {
            "text": "To improve query performance, create a materialized view to use incremental refresh",
            "description": "A materialized view contains a precomputed result set, based on an SQL query over one or more base tables. Materialized views are especially useful for speeding up queries that are predictable and repeated. Instead of performing resource-intensive queries against large tables (such as aggregates or multiple joins), applications can query a materialized view and retrieve a precomputed result set. From the user standpoint, the query results are returned much faster compared to when retrieving the same data from the base tables.<br><br>To complete refresh of the materialized views with minimal impact to active workloads in your cluster, create a materialized view that can perform an incremental refresh in order to quickly identify the changes to the data in the base tables since the last refresh and updates the data in the materialized view.<br>Blogs: <br><ul><a href='https://aws.amazon.com/blogs/big-data/speed-up-your-elt-and-bi-queries-with-amazon-redshift-materialized-views/' target='_blank'>Speed up your ELT and BI queries with Amazon Redshift materialized views</a><a href='https://aws.amazon.com/blogs/big-data/optimize-your-analytical-workloads-using-the-automatic-query-rewrite-feature-of-amazon-redshift-materialized-views/' target='_blank'>Optimize your analytical workloads using the automatic query rewrite feature of Amazon Redshift materialized views</a></ul>Documentation: <br><ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-refresh.html' target='_blank'>Refreshing a materialized view</a></ul>",
            "effort": "Medium"
        },
        "31": {
            "text": "To keep the data accurate and up to date, recreate the materialized view",
            "description": "Due to underlying DDL changes on the base tables used in the materialized view, the materialized view can no longer be refreshed.<br>Documentation: <br><ul><a href='https://docs.aws.amazon.com/redshift/latest/dg/r_STV_MV_INFO.html' target='_blank'>STV_MV_INFO</a></ul>",
            "effort": "Small"
        },
        "32": {
            "text": "To improve query performance on consumer cluster, schedule a period refresh query on consumer cluster",
            "description": "To achieve best in class performance Amazon Redshift consumer clusters caches and incrementally updates block level data (let us refer to this as block metadata) that is queried from the producer cluster (this works even when cluster is paused). The time taken for caching block metadata depends on the rate of the data change on the producer since the respective object(s) were last queried on the consumer. <br><br> As a best practice, when a large table relative to a cluster size (eg 200TB on 4 nodes ra3.16xl) is shared by a producer, one optimization approach that can be done on the consumer cluster is to implement a periodic refresh query such as <br><code>select count(1) from  [consumer_schema].[table_name];</code> to improve the performance of user queries in the consumer cluster.",
            "effort": "Small"
        },
        "33": {
            "text": "To save cost, apply the necessary concurrency scaling usage limit",
            "description": "You can define limits to monitor and control your usage and associated cost of some Amazon Redshift features. You can create daily, weekly, and monthly usage limits, and define actions that Amazon Redshift automatically takes if those limits are reached. Actions include such things as logging an event to a system table to record usage exceeding your defined limits. Other possible actions include raising alerts with Amazon SNS and Amazon CloudWatch to notify an administrator and disabling further usage to control costs.<br>A concurrency scaling limit specifies the threshold of the total amount of time used by concurrency scaling in 1-minute increments.<br><br>Blogs:<ul><a href='https://aws.amazon.com/blogs/big-data/manage-and-control-your-cost-with-amazon-redshift-concurrency-scaling-and-spectrum/' target='_blank'>Manage and control your cost with Amazon Redshift Concurrency Scaling and Spectrum</a></ul>Documentation: <ul><a href='https://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-usage-limits.html' target='_blank'>Managing usage limits in Amazon Redshift</a></ul>",
            "effort": "Small"
        },
        "34": {
            "text": "To improve query performance, create a materialized view to use incremental refresh in the producer cluster and share the materialized view to the consumer cluster(s)",
            "description": "Materialized views (MVs) provide a powerful route to precompute complex aggregations for use cases where high throughput is needed, and you can directly share a materialized view object via data sharing as well.<br><br>For materialized views built on tables where there are frequent write operations, it is ideal to create the materialized view object on the producer itself and share the view. This method gives us the opportunity to centralize the management of the view on the producer cluster itself.<br><br>For slowly changing data tables, you can share the table objects directly and build the materialized view on the shared objects directly on the consumer. This method gives us the flexibility of creating a customized view of data on each consumer according to your use case.<br><br>This can help optimize the block metadata download and caching times in the data sharing query lifecycle. This also helps in materialized view refreshes because Redshift does not support incremental refresh for MVs built on shared objects.<br>Blogs: <br><ul><a href='https://aws.amazon.com/blogs/big-data/speed-up-your-elt-and-bi-queries-with-amazon-redshift-materialized-views/' target='_blank'>Speed up your ELT and BI queries with Amazon Redshift materialized views</a><a href='https://aws.amazon.com/blogs/big-data/amazon-redshift-data-sharing-best-practices-and-considerations/' target ='_blank'>Amazon Redshift data sharing best practices and considerations</a></ul>",
            "effort": "Medium"
        },
        "35": {
            "text": "For better price performance, leverage Redshift serverless.",
            "description": "Redshift serverless provides the same or better price performance when compared to on demand especially with intermittent workloads. This is possible because of its ability to auto pause and resume  so you only pay for the times the cluster is running.<br><br>Documentation:<ul><a href=https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-serverless.html target='_blank'>What is Amazon Redshift Serverless?</a><a href=https://docs.aws.amazon.com/redshift/latest/mgmt/serverless-billing.html target='_blank'>Serverless Billing</a></ul>Demo Video:<ul><a href=https://www.youtube.com/watch?v=XcRJjXudIf8 target='_blank'>Getting started with Amazon Redshift Serverless</a><a href=https://www.youtube.com/watch?v=JBPpmMq9OS8 target='_blank'>Amazon Redshift Serverless - Pay for Use Cost Calculation</a></ul>",
            "effort": "Medium"
        },
        "36": {
            "text": "To avoid 'connection limit' errors and release unused resources, set idle session timeout and clean up idle sessions when needed.",
            "description": "In Amazon Redshift, all connections to your cluster are maintained by the leader node, and there is a maximum limit for active connections. When there are too many active connections in your Amazon Redshift cluster, you may receive an error. See below documentation on how to identify and kill idle sessions.  By default, Redshift applies a default of 4 hours to kill idle session and 6 hours to kill idle transactions.  To override this behavior at the user-level see the ALTER USER documentation. <br><br>Knowledge Center:<ul><a href=https://aws.amazon.com/premiumsupport/knowledge-center/redshift-intermittent-connectivity/ target='_blank'>Connection Limit Error</a></ul>Documentation:<ul><a href=https://docs.aws.amazon.com/redshift/latest/dg/r_ALTER_USER.html#r_ALTER_USER-synopsis target='_blank'>ALTER USER</a><a href=https://docs.aws.amazon.com/redshift/latest/mgmt/amazon-redshift-limits.html target='_blank'>Redshift Limits</a></ul>",
            "effort": "Small"
        },
        "37": {
            "text": "For increased stability, use the 'trailing' track for production workloads.",
            "description": "To reduce the likelihood of a patch degradation and to improve the chances of global compile cache re-use, set your cluster to the Trailing track<br>Documentation:<ul> <a href=https://docs.aws.amazon.com/redshift/latest/APIReference/API_MaintenanceTrack.html target='_blank'>Maintenance Tracks</a></ul>",
            "effort": "Small"
        },
        "38": {
            "text": "To reduce processing skew, perform a classic resize to rebalance your data.",
            "description": "Performing an elastic resize on your Amazon Redshift cluster can unevenly distribute data slices across your nodes because Redshift maintains the same number of data slices in the target configuration. The result of this uneven distribution is processing (CPU) skew and less then optimized query performance. This can be corrected by executing a Classic Resize.<br><br>Documentation: <ul><a href=https://aws.amazon.com/premiumsupport/knowledge-center/redshift-elastic-resize/ target='_blank'>Elastic Resize</a><a href=https://docs.aws.amazon.com/redshift/latest/mgmt/managing-cluster-operations.html#classic-resize-faster target='_blank'>Classic Resize</a></ul>",
            "effort": "Large"
        }
    },
    "Next Actions": []
}