AWSTemplateFormatVersion: '2010-09-09'
Description: Nested Stack for Lambdas L1

Parameters:
  RedshiftClusterEndpoint:
    Description: The endpoint of the evaluated Redshift Cluster.
    Type: String
    Default: redshift-cluster-1.ccmrh4x1dn6o.us-east-1.redshift.amazonaws.com:5439/sample_data_dev
  DbUsername:
    Description: The username of a superuser already created
    Type: String
    Default: ard_user
  ScriptPath:
    Description: The path for SQL scripts
    Type: String
    Default: scripts/
  ConfigPath:
    Description: The path for Config Files scripts
    Type: String
    Default: config/
  ResultPath:
    Description: The path for results
    Type: String
    Default: result/
  HTMLPath:
    Description: The path for HTML output of the review
    Type: String
    Default: html/
  ErrorPath:
    Description: The path for error files (internal - for debugging)
    Type: String
    Default: error/
  ManifestPath:
    Description: The path for the workflow manifests (internal)
    Type: String
    Default: manifest/
  S3BucketName:
    Description: The bucket name of a local S3 bucket for artifacts
    Type: String
    Default: reignite24-local-266726630905-us-east-1
  S3BucketNameGlobal:
    Description: The bucket name of a global S3 bucket for artifacts
    Type: String
    Default: reignite24-global-266726630905-us-east-1
  SNSTopic:
    Type: String
    Description: An SNS Topic for alerting about the problems 
    Default:  arn:aws:sns:us-east-1:266726630905:redshift_review_error_topic_standard




Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Input Parameters
        Parameters:
          - RedshiftClusterEndpoint
          - DbUsername
          - S3BucketName
          - ScriptPath
          - ResultPath
          - HTMLPath
          - ErrorPath
          - ManifestPath
          - S3BucketNameGlobal
          - SNSTopic
        

Resources:
  LambdaWorkflowRole:
    Type: AWS::IAM::Role
    Properties: 
      Description: IAM Role for LambdaWorkflow
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action: sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  LambdaCreateOutputRole:
    Type: AWS::IAM::Role
    Properties: 
      Description: IAM Role for LambdaCreateOutput
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action: sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: lamrol-OutputAccessPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:ListBucket
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}/*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ErrorPath}*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ResultPath}*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ErrorPath}
                  - !Sub arn:aws:s3:::${S3BucketName}/${ResultPath}
                  - !Sub arn:aws:s3:::${S3BucketName}
        - PolicyName: lamrol-XRayPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - xray:PutTraceSegments
                  - xray:PutTelemetryRecords
                  - xray:GetSamplingRules
                  - xray:GetSamplingTargets
                Resource:
                  - '*'

  LambdaCreateHTMLRole:
    Type: AWS::IAM::Role
    Properties: 
      Description: IAM Role for LambdaCreateHTML
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action: sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: lamrol-OutputAccessPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:ListBucket
                  - s3:GetObject
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}/*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ErrorPath}*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ResultPath}*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ConfigPath}*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ErrorPath}
                  - !Sub arn:aws:s3:::${S3BucketName}/${ResultPath}
                  - !Sub arn:aws:s3:::${S3BucketName}/${ConfigPath}
                  - !Sub arn:aws:s3:::${S3BucketName}

        - PolicyName: lamrol-XRayPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - xray:PutTraceSegments
                  - xray:PutTelemetryRecords
                  - xray:GetSamplingRules
                  - xray:GetSamplingTargets
                Resource:
                  - '*'


  LambdaProcessResultsRole:
    Type: AWS::IAM::Role
    Properties: 
      Description: IAM Role for LambdaProcessResults
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action: sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: lamrol-OutputAccessPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:PutObject
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}/*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ManifestPath}*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ErrorPath}*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ManifestPath}
                  - !Sub arn:aws:s3:::${S3BucketName}/${ResultPath}
                  - !Sub arn:aws:s3:::${S3BucketName}
        - PolicyName: lamrol-XRayPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - xray:PutTraceSegments
                  - xray:PutTelemetryRecords
                  - xray:GetSamplingRules
                  - xray:GetSamplingTargets
                Resource:
                  - '*'


  LambdaCopyScriptsRole:
    Type: AWS::IAM::Role
    Properties: 
      Description: IAM Role for lambda to copy scripts
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: copy-inputAccessPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketNameGlobal}/*
                  - !Sub arn:aws:s3:::${S3BucketNameGlobal}/${ScriptPath}*
                  - !Sub arn:aws:s3:::${S3BucketNameGlobal}/${ScriptPath}
                  - !Sub arn:aws:s3:::${S3BucketNameGlobal}
        - PolicyName: copy-outputAccessPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetBucketLocation
                  - s3:ListBucket
                  - s3:PutObject
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}/*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ScriptPath}*
                  - !Sub arn:aws:s3:::${S3BucketName}/${ScriptPath}
                  - !Sub arn:aws:s3:::${S3BucketName}
        - PolicyName: LambdaCloudFormationPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:*
                Resource:
                  - !Sub arn:aws:s3:::cloudformation-custom-resource-response-${AWS::Region}
                  - !Sub arn:aws:s3:::cloudformation-waitcondition-${AWS::Region}
                  - !Sub arn:aws:s3:::cloudformation-custom-resource-response-${AWS::Region}/*
                  - !Sub arn:aws:s3:::cloudformation-waitcondition-${AWS::Region}/*
  
  LambdaHTMLLayer:
    Type: AWS::Lambda::LayerVersion
    Properties:
      CompatibleArchitectures: 
        - x86_64
      CompatibleRuntimes: 
        - python3.12
        - python3.11
        - python3.10
        - python3.9
      Content: 
        S3Bucket: !Ref S3BucketNameGlobal
        S3Key: "layer/pandasqllayer-68fa1979-c520-4440-8bbc-4963df29e912.zip"
      Description: Layer for  CreateHTMLLambda
      LicenseInfo: MIT
  
  PermissionHTMLLayer:
    Type: AWS::Lambda::LayerVersionPermission
    DependsOn: 
      - LambdaHTMLLayer
    Properties:
      Action: lambda:GetLayerVersion
      LayerVersionArn: !Ref LambdaHTMLLayer
      Principal: '*'


  LambdaWorkflow:
    Type: AWS::Lambda::Function
    Properties: 
      Description: Lambda to execute the step function
      Handler: index.lambda_handler
      Role: !GetAtt LambdaWorkflowRole.Arn
      Timeout: 60
      Runtime: python3.10
      Architectures:
        - x86_64
      Layers:
        - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:71
      Code:
        ZipFile: |
          import boto3
          import traceback
          import json
          from aws_lambda_powertools import Logger
          from aws_lambda_powertools.utilities.typing import LambdaContext
          import uuid


          logger = Logger()
          s3 = boto3.client( "s3" )

          @logger.inject_lambda_context
          def lambda_handler(event, context):
              logger.info(event)

              uuid4 = uuid.uuid4()
              uuidstr = str(uuid4).split("-")[0]
              
              status = event.get("Status")
              bucket = event.get("S3BucketName")
              query = event.get("Query")
              workflow = event.get("QueryList")
              script = query.get("Script")
              timets = query.get("Timestamp")
              (segment,subseg) = query.get("Step_ID")

              # 
              next = 0
              for step in workflow:
                  #print(step)
                  if next == 1:
                      next_query = workflow[step]
                      return {
                              'statusCode': 200,
                              'body': {
                                  "Status" : "INITIAL",
                                  "QueryList": workflow,
                                  "Query": next_query,
                                  "S3BucketName": bucket
                              },
                              'continue': True
                          }
                  if step == script:
                      if status == "FINISHED":
                          workflow[step]["Status"] = status
                      elif status == "INITIAL":
                          print("ERROR. INITIAL")
                          workflow[step]["Status"] = "FAILED"
                      else:
                          print(status)
                          workflow[step]["Status"] = "FAILED"
                      next = 1
            
              return {
                  'statusCode': 200,
                  'body': {
                      "QueryList": workflow,
                      "Status" : "COMPLETED",
                      "Query": {
                          "OutputLocation": f"manifest/{timets}/{uuidstr}-workflow-{segment}-{subseg}.manifest"
                      },
                      "S3BucketName": bucket
                  },
                  'continue': False
              }


  LambdaCreateOutput:
    Type: AWS::Lambda::Function
    Properties: 
      Description: Lambda to execute the step function
      Handler: index.lambda_handler
      Role: !GetAtt LambdaCreateOutputRole.Arn
      Timeout: 60
      Runtime: python3.10
      Architectures:
        - x86_64
      Layers:
        - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:71
      Environment:
        Variables: 
          BUCKET_NAME: !Ref S3BucketName
          RESULT_PREFIX: !Ref ResultPath
          POWERTOOLS_LOG_LEVEL: INFO
          POWERTOOLS_SERVICE_NAME: LambdaRolesL1
      Code:
        ZipFile: |
          import json
          import io
          import csv
          import boto3
          import os
          from aws_lambda_powertools import Logger
          from aws_lambda_powertools.utilities.typing import LambdaContext
          logger = Logger()
          s3 = boto3.client('s3')
          def create_csv(json_result):
              column_metadata = json_result['ColumnMetadata']
              column_names = [column['name'] for column in column_metadata]
              records = json_result['Records']
              # Write the CSV data to an in-memory buffer
              csv_buffer = io.StringIO()
              writer = csv.writer(csv_buffer, delimiter ="|")
              writer.writerow(column_names)
              if  records != []:
                  for record in records:
                      row = []
                      for value in record:
                          if 'stringValue' in value:
                              row.append(value['stringValue'])
                          elif 'longValue' in value:
                              row.append(str(value['longValue']))
                      writer.writerow(row)
              return csv_buffer.getvalue()
          @logger.inject_lambda_context
          def lambda_handler(event, context):
              # TODO implement
              print(event)
              result_bucket = os.environ["BUCKET_NAME"]
              result_prefix = os.environ["RESULT_PREFIX"]
              timestamp = event["timestamp"]
              try:
                  list = s3.list_objects_v2(Bucket=result_bucket, Prefix=f"{result_prefix}{timestamp}/")
                  for obj in list.get('Contents', []):
                      if "json" in obj["Key"]:
                          #print(obj['Key'])#read all the results
                          response = s3.get_object(Bucket=result_bucket, Key=obj['Key'])
                          strii = response['Body'].read().decode('utf-8')
                          intm = json.loads(strii)
                          for keyk in intm["Records"]:
                              print(keyk)
                              for kk in keyk:
                                  print(kk)
                                  for k in kk:
                                      if k =="stringValue":
                                          if kk[k] == "":
                                            kk[k] = "empty"
                                          else:
                                              kk[k] = str(kk[k]).replace("\n"," ").replace("\t"," ").replace("\"","").replace("'","").replace("`","").replace('"','').replace("|","-")
                                  for k in kk:
                                      if k =="isNull":
                                          print(kk[k] )
                                          print(type(kk[k]))
                                          if bool(kk[k]) == True:
                                            print("TRUE")
                                            kk[k] = "empty"
                                          else:
                                              kk[k] = str(kk[k]).replace("\n"," ").replace("\t"," ").replace("\"","").replace("'","").replace("`","").replace('"','').replace("|","-")

                          strii = json.dumps(intm).replace ("isNull","stringValue")
                          intm = json.loads(strii)
                          csv_buf = create_csv(intm).replace("\"","").replace("'","").replace("`","").replace('"','')
                          result = s3.put_object(Bucket=result_bucket, Key=obj['Key'].replace("json","csv"), Body=csv_buf)
                  return {
                          'statusCode': 200,
                          'message': 'CSV files created'
                          }
              except Exception as e:
                  print(repr(e))
                  return {
                          'statusCode': 400,
                          'message': 'problem in processing CSV files '
                          }

  LambdaProcessResults:
    Type: AWS::Lambda::Function
    Properties: 
      Description: Lambda to execute the step function
      Handler: index.lambda_handler
      Role: !GetAtt LambdaProcessResultsRole.Arn
      Timeout: 60
      Runtime: python3.10
      Architectures:
        - x86_64
      Layers:
        - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:71
      Environment:
        Variables: 
          BUCKET_NAME: !Ref S3BucketName
          RESULT_PREFIX: !Ref ManifestPath
          POWERTOOLS_LOG_LEVEL: INFO
          POWERTOOLS_SERVICE_NAME: LambdaRolesL1
      Code:
        ZipFile: |
          import json
          import io
          import os
          import csv
          import boto3
          from datetime import datetime
          from aws_lambda_powertools import Logger
          from aws_lambda_powertools.utilities.typing import LambdaContext
          logger = Logger()
          bucket = os.environ["BUCKET_NAME"]
          manifest_prefix = os.environ["RESULT_PREFIX"]
          s3 = boto3.client("s3")
          def manifest_list(segment, step, timets):
              try:
                  list = s3.list_objects_v2(Bucket=bucket, Prefix=f"{manifest_prefix}{timets}/")
                  manifest_list = []
                  for obj in list.get('Contents', []):
                      if f"workflow-{segment}-{step}.manifest" in obj["Key"] :
                          manifest_list.append(obj)
                  return manifest_list
              except Exception as e:
                print (repr(e))
                return []
          @logger.inject_lambda_context
          def lambda_handler(event, context):
              print(event)
              bucket = event.get("S3BucketName")
              print(bucket)
              
              print(type(event))
              print(event.keys())
              
              result = event.get("input").get("Output")
              print(result)
              
              workflow = json.loads(result)
              query = workflow.get("Query")

              #"OutputLocation": "manifest/1720404229/55f98015-workflow-18-14.manifest"

              querylist = workflow.get("QueryList")
              location = query["OutputLocation"]
              filepath = location.split(".")[0]
              filename = filepath.split("/")[-1]
              timets = filepath.split("/")[1]
              segment = filename.split("-")[2]
              step = filename.split("-")[3]  

              print(timets)
              print(segment)
              print(step)
              
              len_mf = len(manifest_list(segment, step, timets))
              
              print(len_mf)
              # check if there are more than 1 manifest
              if len_mf >= 3 or len_mf == 0:
                end_state = True
              else:
                end_state = False
              # create a new workflow from failed states
              next_wf = {}
              for step in querylist:
                if querylist[step]["Status"] == "FINISHED":
                  pass
                else:
                  next_wf[step] = querylist[step]
                  next_wf[step]["Status"] = "INITIAL" 

                  query = querylist[step]
              # if new workflow is not empty and still another retry can be done
              # return the new workflow to the step function
              if next_wf != {}:
                if end_state == False:
                  return {
                      'statusCode': 200,
                      'body': {
                          "QueryList": next_wf,
                          "Query": query,
                          "S3BucketName": bucket,
                          "Status" : "INITIAL"
                      },
                      'rerun': True,
                      'succeeded': False
                  }
                else:
                  # create a report to be sent to the customer via SNS
                  report = {}
                  for step in next_wf:
                    report[step] = { "Status" : next_wf[step]["Status"]  }
                  return {
                      'statusCode': 400,
                      'body': {
                          'error': {
                                'error_queries': report,
                                'error_message' : "There is a persistent error in executing the listed queries. Please contact the TAM team"
                            } 
                      },
                      'rerun': False,
                      'succeeded': False
                  }
              else:
                return {
                      'body': {'message' : 'success', 'timestamp' : timets},
                      'statusCode': 200,
                      'rerun': False,
                      'succeeded': True

                  }



  LambdaCreateHTML:
    Type: AWS::Lambda::Function
    Properties: 
      Description: Lambda to create HTML output for the review
      Handler: index.lambda_handler
      Role: !GetAtt LambdaCreateHTMLRole.Arn
      Timeout: 60
      Runtime: python3.10
      Architectures:
        - x86_64
      Layers:
        - !Ref LambdaHTMLLayer
        - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:71
      Environment:
        Variables: 
          BUCKET_NAME: !Ref S3BucketName
          RESULT_PREFIX: !Ref ResultPath
          HTML_PATH: !Ref HTMLPath
          CONFIG_PATH: !Ref ConfigPath
          POWERTOOLS_LOG_LEVEL: INFO
          POWERTOOLS_SERVICE_NAME: LambdaRolesL1
      Code:
        ZipFile: |
          import json
          import io
          #import awswrangler as wr
          import boto3
          import csv
          import os
          import pandas as pd
          import pandasql as ps
          from aws_lambda_powertools import Logger
          from aws_lambda_powertools.utilities.typing import LambdaContext

          s3 = boto3.client('s3')
          logger = Logger()
          @logger.inject_lambda_context
          def lambda_handler(event, context):
              logger.info(event)
              bucket_name = os.environ['BUCKET_NAME']

              result_prefix = os.environ['RESULT_PREFIX']
              html_path = os.environ['HTML_PATH']
              config_path = os.environ['CONFIG_PATH']

              scripts = event["scripts"]
              max_time =  int(event["max_time"])
              wait = int(event["wait"])
              wait_time = int(event["wait_time"])
              timestamp = str(int(event["timestamp"]))

              wait -= 1

              files =[]
              script_names = []
              object_list = s3.list_objects_v2(Bucket=bucket_name, Prefix=f"{result_prefix}{timestamp}")

              logger.info(object_list)
              
              
              for obj in object_list.get('Contents', []):
                  if "csv" in obj["Key"]:
                      files.append(obj)
                      script_names.append(obj["Key"].split("/")[-1])
                      
              script_string = ",".join(script_names)
              logger.info(script_string)
              
              for script in scripts:
                  if script in script_string:
                    pass
                  else:
                    return {
                        'statusCode': 200,
                        'body': {"scripts" : scripts, "max_time" : max_time, 
                                'wait' : wait, "wait_time" : wait_time, "timestamp" : timestamp ,
                                'message' : f'your report was not created due to timeout. Consult data in  s3://{bucket_name}/{html_path}{timestamp}/'}
                    }

              data = s3.get_object( Bucket=bucket_name, Key=f'{config_path}review_config.json')
              json_data = json.load(data['Body'])
              Recommendations= json_data['Recommendations']

              tabcount = 1

              htmldata = s3.get_object( Bucket=bucket_name, Key=f'{config_path}review_result.html')

              html_data=htmldata['Body'].read().decode()

              for file in files:
                  textlist = []
                  desclist = []
                  signallist = []
                  criterialist = []
                  countlist = []

                  filename = file.get("Key")
                  sectionname = filename.split("/")[-1].split(".")[0]
                  print(sectionname)

                  section = json_data['Sections'][sectionname]

                  dbdata= s3.get_object( Bucket=bucket_name, Key=filename)

                  df = pd.read_csv(dbdata.get("Body"), sep='|')
                  #dfmin=df['storage_utilization_pct'].min()
                  print(df)
                  temptext = ""
                  tempdesc = ""
                  columns=[ 'Count', 'Signal','Criteria', 'Description', 'Recommendation']
                  for signal in section['Signals']:
                      print(signal)
                      print(signal['Criteria'])
                      if(not signal['Criteria'].startswith('skip')):
                          print(signal['Criteria'])
                          if(signal['Criteria'].startswith('select')):
                              iPos=signal['Criteria'].find('||')
                              if(iPos> -1):
                                  dfres1= ps.sqldf(signal['Criteria'][0:iPos-1],  locals() ) 
                                  strTemp=str(dfres1.iat[0,0]) + signal['Criteria'][iPos+2:]
                                  if(eval(strTemp)==True):
                                      dfres=dfres1
                                  else:
                                      dfres=pd.DataFrame()
                              else:
                                  dfres= ps.sqldf(signal['Criteria'],  locals() )
                          else:    
                              dfres=df.query(signal['Criteria'])
                          if (not dfres.empty):
                              signallist.append(signal['Signal'])
                              criterialist.append(signal['Criteria'])
                              countlist.append(len(dfres))
                              temptext=""
                              tempdesc=""
                              for Recommendation in signal['Recommendation']:
                                  temptext= temptext + Recommendations[Recommendation]['text'] + '<br>'
                                  tempdesc=tempdesc + Recommendations[Recommendation]['description'] + '<br>'
                              textlist.append(temptext)
                              desclist.append(tempdesc)


                  dfresult = pd.DataFrame(list(zip(countlist, signallist, criterialist, textlist, desclist)), columns=columns)
                  os.chdir('/tmp')
                  # with pd.ExcelWriter('output.xlsx') as writer: 
                  #     dfresult.to_excel(writer, sheet_name=filename, index=False)
                  #     dfresult.to_excel(writer, sheet_name='test1')
                  # workbook = writer.book
                  # worksheet = writer.sheets[filename]
                  # worksheet.column_dimensions['A'].width = 100
                  # worksheet.column_dimensions['B'].width = 60
                  # #worksheet.column_dimensions['A'].auto_fit = True
                  # worksheet.column_dimensions['A'].wrap_text = True
                  
                  # workbook.save('output.xlsx')
                  
                  # s3.upload_file('/tmp/output.xlsx', 'lavatest2', 'output.xlsx')
                  
                  if (not dfresult.empty):
                      htmloutput = dfresult.to_html(index=False, justify='left', escape=False, render_links=True)
                  else:
                      htmloutput = "No recommendations at this time"
                      
                  if (tabcount < 10):
                      strTabCount="0" + str(tabcount)
                  else:
                      strTabCount= str(tabcount)    
                  print("tabtext" + str(tabcount))
                  html_data = html_data.replace("tabtext_" + strTabCount, htmloutput)
                  print('f ' + filename)
                  html_data = html_data.replace("Tab_"+ strTabCount, filename)
                  
                  
                  tabcount=tabcount + 1
                  
                # s3.put_object(Bucket='lavatest2', Key='output.xlsx')
                  
                # wr.s3.to_excel(df, 's3://lavatest2/results.xlsx', sheet_name='noderesult')
                  
                  
                  #with pd.ExcelWriter('s3://lavatest2/results.xlsx', mode='a') as writer:
                  #    df.to_excel(writer, sheet_name='noderesult1')
                  #xldata = output.getvalue()
                  #s3.put_object(Bucket='lavatest2', Key='result.xlsx', Body=xldata, )
              # write html to file
              text_file = open("/tmp/output.html", "w")
              text_file.write(html_data)
              text_file.close()
              s3.upload_file('/tmp/output.html', bucket_name, f'{html_path}{timestamp}/review.html')
              return {
                  'statusCode': 200,
                  'body': {"scripts" : scripts, "max_time" : max_time, 
                                'wait' : 0, "wait_time" : 0, 
                                'message' : f'Your report is ready at s3://{bucket_name}/{html_path}{timestamp}/review.html'}
              }

  LambdaCopyScriptsFunction:
    Type: AWS::Lambda::Function
    Properties: 
      Description: Lambda to execute the step function
      Handler: index.lambda_handler
      Runtime: python3.10
      Architectures:
        - x86_64
      Layers:
        - !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:71
      Role: !GetAtt LambdaCopyScriptsRole.Arn
      Timeout: 600
      Environment:
        Variables: 
          SOURCE_BUCKET: !Ref S3BucketNameGlobal
          SCRIPT_PREFIX: !Ref ScriptPath
          DESTINATION_BUCKET: !Ref S3BucketName
          CONFIG_PREFIX: !Ref ConfigPath
          POWERTOOLS_LOG_LEVEL: INFO
          POWERTOOLS_SERVICE_NAME: LambdaRolesL1
      Code:
        ZipFile: |
          import boto3
          import traceback
          import json
          import os
          import cfnresponse
          from aws_lambda_powertools import Logger
          from aws_lambda_powertools.utilities.typing import LambdaContext
          logger = Logger()
          @logger.inject_lambda_context
          def lambda_handler(event, context):
              source_bucket = os.environ['SOURCE_BUCKET']
              destination_bucket = os.environ['DESTINATION_BUCKET']
              config_prefix = os.environ['CONFIG_PREFIX']
              script_prefix = os.environ['SCRIPT_PREFIX']
              print(event)
              s3 = boto3.client('s3')
              response ={}
              if event['RequestType'] != 'Delete':
                  try:
                      response = s3.list_objects_v2(Bucket=source_bucket, Prefix=script_prefix)
                      for obj in response.get('Contents', []):
                          copy_source = {'Bucket': source_bucket, 'Key': obj['Key']}
                          response = s3.copy_object(CopySource=copy_source, Bucket=destination_bucket, Key=obj['Key'])
                      try:
                          response = s3.list_objects_v2(Bucket=source_bucket, Prefix=config_prefix)
                          for obj in response.get('Contents', []):
                              copy_source = {'Bucket': source_bucket, 'Key': obj['Key']}
                              response = s3.copy_object(CopySource=copy_source, Bucket=destination_bucket, Key=obj['Key'])
                      except:
                          print(traceback.format_exc())
                          cfnresponse.send(event, context, cfnresponse.FAILED, {})
                          raise
                  except:
                      print(traceback.format_exc())
                      cfnresponse.send(event, context, cfnresponse.FAILED, {})
                      raise
                  try:
                      response = s3.list_objects_v2(Bucket=source_bucket, Prefix=config_prefix)
                      for obj in response.get('Contents', []):
                          copy_source = {'Bucket': source_bucket, 'Key': obj['Key']}
                          response = s3.copy_object(CopySource=copy_source, Bucket=destination_bucket, Key=obj['Key'])
                  except:
                      print(traceback.format_exc())
                      cfnresponse.send(event, context, cfnresponse.FAILED, {})
                      raise
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})

Outputs: 
    RedshiftClusterReference:
      Description: "The Endpoint of the Redshift Cluster being referenced."
      Value: !Ref RedshiftClusterEndpoint

    LambdaCopyScriptsFunctionName:
      Description: The FunctionName of the lambda that copy scripts
      Value: !Ref LambdaCopyScriptsFunction


    LambdaWorkflowFunctionName:
      Description: The FunctionName of the lambda that drives the workflow
      Value: !Ref LambdaWorkflow

    LambdaCreateOutputFunctionName:
      Description: The FunctionName of the lambda that creates the output
      Value: !Ref LambdaCreateOutput

    LambdaCreateHTMLFunctionName:
      Description: The FunctionName of the lambda that creates the  html report
      Value: !Ref LambdaCreateHTML

    LambdaProcessResultsFunctionName:
      Description: The FunctionName of the lambda that processes the results
      Value: !Ref LambdaProcessResults